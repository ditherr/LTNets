{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **The Tokenizers**\n",
        "**Train** a brand **new tokenizer on a corpus of texts**, so it can then be **used to pretrain a language model**. This will all be done with the help of the ğŸ¤— `Tokenizers` library, which provides the **â€œfastâ€** tokenizers in the ğŸ¤— Transformers library. Weâ€™ll take a close look at the features that this library provides, and explore how the fast tokenizers differ from the **â€œslowâ€** versions.\n",
        "\n",
        "**Topics:**\n",
        "- How to **train a new tokenizer** similar to the one used by a given checkpoint on a new corpus of texts\n",
        "- The special features of **fast tokenizers**\n",
        "- The differences between the **three main subword tokenization algorithms** used in NLP today\n",
        "- How to **build a tokenizer from scratch** with the ğŸ¤— Tokenizers library and train it on some data."
      ],
      "metadata": {
        "id": "Xsx3lQF9_7fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training a New Tokenizer from an old one**\n",
        "**If a language model is not available in the language you are interested in**, *or* **if your corpus is very different from the one your language model was trained on**, you will most likely want to **retrain the model from scratch using a tokenizer adapted to your data**. That will require training a new tokenizer on your dataset.\n",
        "\n",
        "\n",
        "Most Transformers models use a *`subword tokenization algorithm`*. To identify which subwords are of interest and occur most frequently in the corpus at hand, the **`tokenizer` needs to take a hard look at all the texts in the corpus** â€” a process we call *training*. The exact rules that govern this training depend on **the type of tokenizer used** (*go over 3 main algorithm*)."
      ],
      "metadata": {
        "id": "yI8jlkh2_-BP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "âš ï¸ **Training a tokenizer** is not the same as training a model! Model training uses *stochastic gradient descent* to make the loss a little bit smaller for each batch. Itâ€™s randomized by nature (meaning you have to set some seeds to get the same results when doing the same training twice).\n",
        "\n",
        "âš ï¸ **Training a tokenizer** is a statistical process that tries to **identify which subwords are the best to pick for a given corpus**, *and* **the exact rules used to pick them depend on the tokenization algorithm**. Itâ€™s deterministic, meaning you *always get the same results when training with the same algorithm on the same corpus*."
      ],
      "metadata": {
        "id": "bfDp-UZBJRUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Assembling a corpus**\n",
        "Thereâ€™s a very simple API in ğŸ¤— Transformers that you can use to **train a new tokenizer with the same characteristics as an existing one**: `AutoTokenizer.train_new_from_iterator()`.\n",
        "\n",
        "To see this in action, letâ€™s say we want to **train `GPT-2` from scratch, but in a language other than *English*.** Our *first task* will be to **gather lots of data in that language in a training corpus**. To provide examples everyone will be able to understand, we wonâ€™t use a language like Russian or Chinese here, but rather a specialized English language: ***Python code.***"
      ],
      "metadata": {
        "id": "wRk9y8PwJR1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ğŸ¤— Datasets library can help us assemble a corpus of Python source code. Weâ€™ll use the usual `load_dataset()` function to download and cache the `CodeSearchNet` dataset. This dataset was created for the CodeSearchNet challenge and **contains millions of functions from open source libraries on GitHub in several programming languages**. Here, we will load the **Python** part of this dataset:"
      ],
      "metadata": {
        "id": "aOTMl7bgPXp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# This can take a few minutes to load, so grab a coffee or tea while you wait!\n",
        "raw_datasets = load_dataset(\"code_search_net\", \"python\")"
      ],
      "metadata": {
        "id": "_vSoN8-Seh-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets[\"train\"]\n",
        "\n",
        "## Output\n",
        "# Dataset({\n",
        "#     features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language',\n",
        "#       'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name',\n",
        "#       'func_code_url'\n",
        "#     ],\n",
        "#     num_rows: 412178\n",
        "# })"
      ],
      "metadata": {
        "id": "ViXgyEzyekyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets[\"train\"][0]"
      ],
      "metadata": {
        "id": "AK_-VwKseqCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here. weâ€™ll just use the `whole_func_string` column to train our tokenizer."
      ],
      "metadata": {
        "id": "mF5JJKdI_pt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])\n",
        "\n",
        "## Output\n",
        "# def handle_simple_responses(\n",
        "#       self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):\n",
        "#     \"\"\"Accepts normal responses from the device.\n",
        "\n",
        "#     Args:\n",
        "#       timeout_ms: Timeout in milliseconds to wait for each response.\n",
        "#       info_cb: Optional callback for text sent from the bootloader.\n",
        "\n",
        "#     Returns:\n",
        "#       OKAY packet's message.\n",
        "#     \"\"\"\n",
        "#     return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms)"
      ],
      "metadata": {
        "id": "BgGkcNmq_q44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing we need to do is **transform the dataset into an *iterator* of lists of texts** â€” for instance, a list of list of texts.\n",
        "\n",
        "Using lists of texts will enable our tokenizer to go faster (training on batches of texts instead of processing individual texts one by one), and it should be an iterator if we want to avoid having everything in memory at once.\n",
        "\n",
        "If your corpus is huge, you will want to take advantage of the fact that *ğŸ¤— `Datasets` does not load everything into RAM but stores the elements of the dataset on disk*."
      ],
      "metadata": {
        "id": "w8E9TgGYAKv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create **list of lists of 1000 texts each** by using a *Python generator*, we can avoid Python loading anything into memory until itâ€™s actually necessary."
      ],
      "metadata": {
        "id": "fZif-GjDAdg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't uncomment the following line unless your dataset is small!\n",
        "# training_corpus = [raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000)]"
      ],
      "metadata": {
        "id": "aUkOhDOBA4Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Using Python Generator\n",
        "training_corpus = (\n",
        "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
        "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
        ")\n",
        "# This code doesnâ€™t fetch any elements of the dataset;\n",
        "# it just creates an object you can use in a Python for loop"
      ],
      "metadata": {
        "id": "Ruii7m5YAxBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The texts will only be loaded when you need them (that is, when youâ€™re at the step of the `for` loop that requires them), and only 1,000 texts at a time will be loaded.\n",
        "\n",
        "The problem with a *generator object* is that it can only be *used once*. So, instead of this giving us the list of the first 10 digits twice, we get them **once and then an empty list**."
      ],
      "metadata": {
        "id": "wfTnHuQpBgu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the function to returns a generator instead\n",
        "def get_training_corpus():\n",
        "    return (\n",
        "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
        "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
        "    )\n",
        "\n",
        "\n",
        "training_corpus = get_training_corpus()"
      ],
      "metadata": {
        "id": "QkdYzITxB1v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or define a generator inside a for loop by using the yield\n",
        "def get_training_corpus():\n",
        "    dataset = raw_datasets[\"train\"]\n",
        "    for start_idx in range(0, len(dataset), 1000):\n",
        "        samples = dataset[start_idx : start_idx + 1000]\n",
        "        yield samples[\"whole_func_string\"]\n",
        "\n",
        "# which will produce the exact same generator as before,\n",
        "# but allows you to use more complex logic than you can in a list comprehension."
      ],
      "metadata": {
        "id": "WPROqU0SCBIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training a new Tokenizer**\n",
        "Now that we have our **corpus** in the form of an *iterator of batches of texts*, we are ready to train a new tokenizer."
      ],
      "metadata": {
        "id": "G-8IlLF8CAiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the tokenizer that want to pair with our model (etc., GPT-2)"
      ],
      "metadata": {
        "id": "DnKpDNphCsPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "9J0DJ-CwCX8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though we are going to train a new tokenizer, itâ€™s a good idea to do this to avoid starting entirely from scratch.\n",
        "\n",
        "This way, we wonâ€™t have to specify anything about the tokenization algorithm or the special tokens we want to use; **our new tokenizer will be exactly the same as GPT-2**, *and* **the only thing that will change is the vocabulary**, which will be **determined by the training on our corpus**."
      ],
      "metadata": {
        "id": "cqPzde4qB09U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Example\n",
        "example = '''def add_numbers(a, b):\n",
        "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
        "    return a + b'''\n",
        "\n",
        "tokens = old_tokenizer.tokenize(example)\n",
        "tokens\n",
        "\n",
        "# ['def', 'Ä add', '_', 'n', 'umbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä \"\"\"', 'Add', 'Ä the', 'Ä two',\n",
        "#  'Ä numbers', 'Ä `', 'a', '`', 'Ä and', 'Ä `', 'b', '`', '.\"', '\"\"', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']"
      ],
      "metadata": {
        "id": "UTOY8GU-DIhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tokenizer has a few special symbols, like `Ä ` and `ÄŠ`, which denote *spaces* and *newlines*, respectively.\n",
        "\n",
        "As we can see, this is not too efficient: the tokenizer returns individual tokens for each space, when it could group together indentation levels (since having sets of four or eight spaces is going to be very common in code). It also split the function name a bit weirdly, not being used to seeing words with the `_` character."
      ],
      "metadata": {
        "id": "9Q9Qx7rlDHPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s train a new tokenizer and see if it solves those issues. For this, weâ€™ll use the method `train_new_from_iterator()`"
      ],
      "metadata": {
        "id": "Z5mdtFQRFnL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
      ],
      "metadata": {
        "id": "Nn8iZh1PFsL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(example)\n",
        "tokens\n",
        "\n",
        "# ['def', 'Ä add', '_', 'numbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠÄ Ä Ä ', 'Ä \"\"\"', 'Add', 'Ä the', 'Ä two', 'Ä numbers', 'Ä `',\n",
        "#  'a', '`', 'Ä and', 'Ä `', 'b', '`.\"\"\"', 'ÄŠÄ Ä Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']"
      ],
      "metadata": {
        "id": "GWpj28rfGXgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we again see the special symbols `Ä ` and `ÄŠ` that denote *spaces* and *newlines*, but we can also see that our tokenizer learned some tokens that are highly specific to a corpus of Python functions: for example, there is a `ÄŠÄ Ä Ä ` token that represents an *indentation*, and a `Ä \"\"\"` token that represents the *three quotes that start a docstring*. The tokenizer also correctly split the function name on `_`."
      ],
      "metadata": {
        "id": "iArab9iTGngu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is quite a compact representation; comparatively, using the plain English tokenizer on the same example will give us a longer sentence:"
      ],
      "metadata": {
        "id": "-RQcr_VLG3Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokens))\n",
        "print(len(old_tokenizer.tokenize(example)))"
      ],
      "metadata": {
        "id": "qzeBaMU0G5jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Another example\n",
        "example = \"\"\"class LinearLayer():\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weight = torch.randn(input_size, output_size)\n",
        "        self.bias = torch.zeros(output_size)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return x @ self.weights + self.bias\n",
        "    \"\"\"\n",
        "tokenizer.tokenize(example)\n",
        "\n",
        "# ['class', 'Ä Linear', 'Layer', '():', 'ÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'init', '__(', 'self', ',', 'Ä input', '_', 'size', ',',\n",
        "#  'Ä output', '_', 'size', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'weight', 'Ä =', 'Ä torch', '.', 'randn', '(', 'input', '_',\n",
        "#  'size', ',', 'Ä output', '_', 'size', ')', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ä self', '.', 'bias', 'Ä =', 'Ä torch', '.', 'zeros', '(',\n",
        "#  'output', '_', 'size', ')', 'ÄŠÄŠÄ Ä Ä ', 'Ä def', 'Ä __', 'call', '__(', 'self', ',', 'Ä x', '):', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ',\n",
        "#  'Ä return', 'Ä x', 'Ä @', 'Ä self', '.', 'weights', 'Ä +', 'Ä self', '.', 'bias', 'ÄŠÄ Ä Ä Ä ']"
      ],
      "metadata": {
        "id": "pjDTxHy8G-ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to the token corresponding to an indentation, here we can also see a token for a double indentation: `ÄŠÄ Ä Ä Ä Ä Ä Ä `. The special Python words like `class`, `init`, `call`, `self`, and `return` are each tokenized as one token, and we can see that as well as splitting on `_` and `.` the tokenizer correctly splits even camel-cased names: LinearLayer is tokenized as `[\"Ä Linear\", \"Layer\"].`"
      ],
      "metadata": {
        "id": "Lg96y4hJHKCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Saving the Tokenizer**\n",
        "To make sure we can use it later, we need to save our new tokenizer. Like for models, this is done with the `save_pretrained()` method"
      ],
      "metadata": {
        "id": "Xlk9IdKwHUTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
      ],
      "metadata": {
        "id": "opiAanxhHk4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will create a new folder named `code-search-net-tokenizer`, which will contain all the files the tokenizer needs to be reloaded.\n",
        "\n",
        "If you want to share this tokenizer with your colleagues and friends, you can upload it to the Hub by logging into your account"
      ],
      "metadata": {
        "id": "D-8kqkUuHpA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n",
        "\n",
        "## Working with terminal\n",
        "# huggingface-cli login"
      ],
      "metadata": {
        "id": "37mbSSY6HrvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Push tokenizer to the Hub**"
      ],
      "metadata": {
        "id": "ugzGO9uyHwV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(\"code-search-net-tokenizer\")"
      ],
      "metadata": {
        "id": "yhyjZwhmHzlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will create a new repository in your namespace with the name `code-search-net-tokenizer`, containing the tokenizer file."
      ],
      "metadata": {
        "id": "VduVXhTeH3Tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load our new Tokenizer\n",
        "# Replace \"huggingface-course\" below with your actual namespace to use your own tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")"
      ],
      "metadata": {
        "id": "LEjCMv24H8Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fast Tokenizers' special powers**"
      ],
      "metadata": {
        "id": "Cbkhg9k-AB1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fast Tokenizers in the `QA Pipeline`**"
      ],
      "metadata": {
        "id": "w1vItWISAGLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **`Normalization` and Pre-Tokenization**"
      ],
      "metadata": {
        "id": "BjtK2BJiAG03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **`Byte-Pair Encoding` Tokenization**"
      ],
      "metadata": {
        "id": "6mIBXvT9AHDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **`WordPiece` Tokenization**"
      ],
      "metadata": {
        "id": "KSJKnjY-AHUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **`Unigram` Tokenization**"
      ],
      "metadata": {
        "id": "WRIvkgefAHkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Building a Tokenizer, block-by-block**"
      ],
      "metadata": {
        "id": "vrPxg2ALAH0G"
      }
    }
  ]
}