{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNaC+nr+FSLHnft9Eh21dcq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b23870baf9614bf999e25aaabe523a1f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c0f804b59a984f82849613a65f732d1f","IPY_MODEL_ea898d791e85481c901ee570ab7e9976","IPY_MODEL_d3667d214c7d4f8ca34d4fcc13408d9e"],"layout":"IPY_MODEL_e68be92e273c40fb954031e6e0d8286a"}},"c0f804b59a984f82849613a65f732d1f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5b816fe8ad849b4a0e982582e554637","placeholder":"​","style":"IPY_MODEL_51db9f11949f42dfb8d9fae55ceaf4fc","value":"config.json: 100%"}},"ea898d791e85481c901ee570ab7e9976":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7b045a4fca94fe8bc02ba7ce59f00cd","max":629,"min":0,"orientation":"horizontal","style":"IPY_MODEL_40b287335ed64939a8bda2d395cca290","value":629}},"d3667d214c7d4f8ca34d4fcc13408d9e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5288dcf4bd544ebeb2c49c43a6542cef","placeholder":"​","style":"IPY_MODEL_b3446108296349b2907f04de782dbc23","value":" 629/629 [00:00&lt;00:00, 28.1kB/s]"}},"e68be92e273c40fb954031e6e0d8286a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5b816fe8ad849b4a0e982582e554637":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51db9f11949f42dfb8d9fae55ceaf4fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7b045a4fca94fe8bc02ba7ce59f00cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40b287335ed64939a8bda2d395cca290":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5288dcf4bd544ebeb2c49c43a6542cef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3446108296349b2907f04de782dbc23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81ecf64ceeab4856a01621ee0ad92742":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a1573852c60e48e8be33d62d7e2c7f04","IPY_MODEL_ec0cfe8665794fb9ab16855aea9342ee","IPY_MODEL_8d7dff107b7f48098bc3afe677ce1c9c"],"layout":"IPY_MODEL_a89774abecd143bcbe64c947a868dc1b"}},"a1573852c60e48e8be33d62d7e2c7f04":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2234b17c0b4465793c6d83d83f605c5","placeholder":"​","style":"IPY_MODEL_252c8c6a280e4db781918ff69f35d7b4","value":"model.safetensors: 100%"}},"ec0cfe8665794fb9ab16855aea9342ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfbf7b96f84741c8b273d6637f5d4dd4","max":267832558,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ef8779524a4c40078b40ee43add9f7a8","value":267832558}},"8d7dff107b7f48098bc3afe677ce1c9c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_383a4d623c6b4b9e8685972deb65945d","placeholder":"​","style":"IPY_MODEL_13d88ece7234485187572109d794046b","value":" 268M/268M [00:02&lt;00:00, 176MB/s]"}},"a89774abecd143bcbe64c947a868dc1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2234b17c0b4465793c6d83d83f605c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"252c8c6a280e4db781918ff69f35d7b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfbf7b96f84741c8b273d6637f5d4dd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef8779524a4c40078b40ee43add9f7a8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"383a4d623c6b4b9e8685972deb65945d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13d88ece7234485187572109d794046b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8e6715a66d846fb850a144e2d8a0577":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_352452c0cc3c4295af6054fe29a80096","IPY_MODEL_fca62fe7a38e487eb7fd42c671453a6f","IPY_MODEL_0fcf295adc594860a53b1a02b0e43d20"],"layout":"IPY_MODEL_b9c0da288568464f9e33114871ddb044"}},"352452c0cc3c4295af6054fe29a80096":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9793a89453941839086c6264d0df126","placeholder":"​","style":"IPY_MODEL_a728dc7404e24323adf443fbd1cb0d05","value":"tokenizer_config.json: 100%"}},"fca62fe7a38e487eb7fd42c671453a6f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fbea12565ca84e8aa9053c1aa018eea3","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a3992c5951d745dc97bbef3072dac790","value":48}},"0fcf295adc594860a53b1a02b0e43d20":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83ad58b2cafb4ee3a7ee007e9662332a","placeholder":"​","style":"IPY_MODEL_3247374292f54cf091455ab75be96a99","value":" 48.0/48.0 [00:00&lt;00:00, 1.75kB/s]"}},"b9c0da288568464f9e33114871ddb044":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9793a89453941839086c6264d0df126":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a728dc7404e24323adf443fbd1cb0d05":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbea12565ca84e8aa9053c1aa018eea3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3992c5951d745dc97bbef3072dac790":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"83ad58b2cafb4ee3a7ee007e9662332a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3247374292f54cf091455ab75be96a99":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07059310cb504155a485114fe04cfeba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f786e597afb44e7f94ff359c015fb87b","IPY_MODEL_4bb7d9f2caf0458881e6f3995a02e292","IPY_MODEL_43b8fef11b9f48829470cab9f9bf1958"],"layout":"IPY_MODEL_4921c3e8937d4dafb9b9c0805ef9590a"}},"f786e597afb44e7f94ff359c015fb87b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55343ea83dc2425c9a85213fb1b34e2c","placeholder":"​","style":"IPY_MODEL_c4587e7c9aba4a2eacc2e36ac0caca6e","value":"vocab.txt: 100%"}},"4bb7d9f2caf0458881e6f3995a02e292":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_df6a02759db145f6b2f8b26125cc5216","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b56831c9e9a5480ca5f43b69c862e024","value":231508}},"43b8fef11b9f48829470cab9f9bf1958":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_595eaccab3c440b7853a10f465eb43d8","placeholder":"​","style":"IPY_MODEL_7ac79db3c7244adcadbd7cec54a53585","value":" 232k/232k [00:00&lt;00:00, 1.17MB/s]"}},"4921c3e8937d4dafb9b9c0805ef9590a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55343ea83dc2425c9a85213fb1b34e2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4587e7c9aba4a2eacc2e36ac0caca6e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df6a02759db145f6b2f8b26125cc5216":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b56831c9e9a5480ca5f43b69c862e024":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"595eaccab3c440b7853a10f465eb43d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ac79db3c7244adcadbd7cec54a53585":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"776acfc657724bafb287b27ac8b7affc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4f0e9a20779f4e82a71f5f046b027138","IPY_MODEL_3687de0a9bbe4570b56137be643fc3ae","IPY_MODEL_0d0caf84270a4f25b4d86eeb0b66b308"],"layout":"IPY_MODEL_6aa1b45360684ce09ad1800e2aea8f84"}},"4f0e9a20779f4e82a71f5f046b027138":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45fdc8968d0a46919332f1e8a062ae4f","placeholder":"​","style":"IPY_MODEL_a008593fd54e463b9eb5400661f2ed56","value":"tokenizer_config.json: 100%"}},"3687de0a9bbe4570b56137be643fc3ae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1cdf187344164236bc5aab7a4d9d6681","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3895fdbae71f49518f7462335b7bc443","value":48}},"0d0caf84270a4f25b4d86eeb0b66b308":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6325dc94a0b74aa48d0c4a64b25c7f4e","placeholder":"​","style":"IPY_MODEL_9e9dcc5c8fd84725a8972b68c84111d4","value":" 48.0/48.0 [00:00&lt;00:00, 2.06kB/s]"}},"6aa1b45360684ce09ad1800e2aea8f84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45fdc8968d0a46919332f1e8a062ae4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a008593fd54e463b9eb5400661f2ed56":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1cdf187344164236bc5aab7a4d9d6681":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3895fdbae71f49518f7462335b7bc443":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6325dc94a0b74aa48d0c4a64b25c7f4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e9dcc5c8fd84725a8972b68c84111d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f918f55c0131404587e82d7babee9133":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_025d53962af3426f9b0cd0eeb18df93b","IPY_MODEL_79132d53b43e425c82b4bb70fa9744de","IPY_MODEL_8e0f666f6476481e8cf0b49654f04fa8"],"layout":"IPY_MODEL_c6edc096ce8a4bae95dbd7f5cfbd1f8c"}},"025d53962af3426f9b0cd0eeb18df93b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c1516cb9dee46acb5d9dd84e98026b8","placeholder":"​","style":"IPY_MODEL_51cff89d57b64c2592f094ecf12f697c","value":"config.json: 100%"}},"79132d53b43e425c82b4bb70fa9744de":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78c13d358c4b47aaa66ba60f9d06fcee","max":629,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a3b6d4a3024a436c88497553320dea59","value":629}},"8e0f666f6476481e8cf0b49654f04fa8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cceb88d1e506484eac81906848a2a753","placeholder":"​","style":"IPY_MODEL_5c019a0b872a465f84d24a8cd99b11b5","value":" 629/629 [00:00&lt;00:00, 12.8kB/s]"}},"c6edc096ce8a4bae95dbd7f5cfbd1f8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c1516cb9dee46acb5d9dd84e98026b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51cff89d57b64c2592f094ecf12f697c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78c13d358c4b47aaa66ba60f9d06fcee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3b6d4a3024a436c88497553320dea59":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cceb88d1e506484eac81906848a2a753":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c019a0b872a465f84d24a8cd99b11b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34ea2fcaa08e47c4a8f67bcb1e9d361e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a8c88ba3342149dd9d50367e9a5f2523","IPY_MODEL_ecc90b2ff1b84e9da9436d96c9e84ac6","IPY_MODEL_048330877a454b748cb7c91c6ffcb9fa"],"layout":"IPY_MODEL_98255685b6e746f0b4c94a638d789426"}},"a8c88ba3342149dd9d50367e9a5f2523":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d1292ae349642cf82f514b997500e8f","placeholder":"​","style":"IPY_MODEL_e988dee3aaf34b31ad2cf14be423b368","value":"vocab.txt: 100%"}},"ecc90b2ff1b84e9da9436d96c9e84ac6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d3980f83d444efa86ffa15995ee9ed2","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0681b69843f540cfb0220e57b4e0b1f9","value":231508}},"048330877a454b748cb7c91c6ffcb9fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_657446939ebc485686ea204efbaaf822","placeholder":"​","style":"IPY_MODEL_8954ef84ebbc4c7587781e1ca4f2e247","value":" 232k/232k [00:00&lt;00:00, 2.93MB/s]"}},"98255685b6e746f0b4c94a638d789426":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d1292ae349642cf82f514b997500e8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e988dee3aaf34b31ad2cf14be423b368":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d3980f83d444efa86ffa15995ee9ed2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0681b69843f540cfb0220e57b4e0b1f9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"657446939ebc485686ea204efbaaf822":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8954ef84ebbc4c7587781e1ca4f2e247":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e1a01234276449ca4a9c80330ab1db8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eecc56f8d02240be9b400f13070bbf6d","IPY_MODEL_abe97cd781024aff8d292ca4694501a4","IPY_MODEL_d25966b7ebea440da4b66907b2ba80ea"],"layout":"IPY_MODEL_fc096d831f7b4770b23f98bf89687f36"}},"eecc56f8d02240be9b400f13070bbf6d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c70c7f49c044621be91f6c418e1e40f","placeholder":"​","style":"IPY_MODEL_817b94a778ff42439f8bb6b8ee28d43e","value":"model.safetensors: 100%"}},"abe97cd781024aff8d292ca4694501a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed853287d49a4d06af19c885f4506a6f","max":267832558,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca66bc7eed464fee97e63db991e80cf7","value":267832558}},"d25966b7ebea440da4b66907b2ba80ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c5ce98daa2b4c85a17df27dab7bee10","placeholder":"​","style":"IPY_MODEL_7564ceeb2d9e412d8d9c090575cfe09a","value":" 268M/268M [00:05&lt;00:00, 115MB/s]"}},"fc096d831f7b4770b23f98bf89687f36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c70c7f49c044621be91f6c418e1e40f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"817b94a778ff42439f8bb6b8ee28d43e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed853287d49a4d06af19c885f4506a6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca66bc7eed464fee97e63db991e80cf7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2c5ce98daa2b4c85a17df27dab7bee10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7564ceeb2d9e412d8d9c090575cfe09a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d9b24b7d844b48348cf2393caeb1b820":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_528304f5f6fc47cbb879c3135cfeb937","IPY_MODEL_3b404c31bb3f49088187f829f2f4f9da","IPY_MODEL_9f24d72b9c0b48b19d1182828fb13be4"],"layout":"IPY_MODEL_6bc375becc464451aedefeac988bbb4a"}},"528304f5f6fc47cbb879c3135cfeb937":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_181e53fed398444d85effd2e0de0451b","placeholder":"​","style":"IPY_MODEL_5c95afbc46a3443e8b455673b069a1b9","value":"config.json: 100%"}},"3b404c31bb3f49088187f829f2f4f9da":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9274bd1b34b44b49e3ba296915bbba7","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d526d3e0ce104bc6bd37e68311ed3716","value":570}},"9f24d72b9c0b48b19d1182828fb13be4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f777b10106bd4a619dce84aec37571d0","placeholder":"​","style":"IPY_MODEL_4798624a302e41a19319710a3aea91e4","value":" 570/570 [00:00&lt;00:00, 6.02kB/s]"}},"6bc375becc464451aedefeac988bbb4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"181e53fed398444d85effd2e0de0451b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c95afbc46a3443e8b455673b069a1b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9274bd1b34b44b49e3ba296915bbba7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d526d3e0ce104bc6bd37e68311ed3716":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f777b10106bd4a619dce84aec37571d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4798624a302e41a19319710a3aea91e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b55ba019a7c34b0388179470c139f985":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_578567ac5e8243c197da2c1b4c7e0339","IPY_MODEL_4eb51034d83a4f989959cbfb4c63f1ac","IPY_MODEL_27de2b1e787945bd804cb477016ce59e"],"layout":"IPY_MODEL_595b0fdd153444e8975296aefb964258"}},"578567ac5e8243c197da2c1b4c7e0339":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b9faa2692f94d69b604fe77cb24de34","placeholder":"​","style":"IPY_MODEL_5b6a920e3145467e81f443688d210948","value":"model.safetensors: 100%"}},"4eb51034d83a4f989959cbfb4c63f1ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f63c388008d4a19b4bc282732af8c3d","max":435755784,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3611ab10989b4ebe80978f32fd8b93f3","value":435755784}},"27de2b1e787945bd804cb477016ce59e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f55da0f6df604b5b9a45f4db14e4c45c","placeholder":"​","style":"IPY_MODEL_6d435d0e7de948b38049a8f81aeaa34e","value":" 436M/436M [00:05&lt;00:00, 77.2MB/s]"}},"595b0fdd153444e8975296aefb964258":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b9faa2692f94d69b604fe77cb24de34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b6a920e3145467e81f443688d210948":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f63c388008d4a19b4bc282732af8c3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3611ab10989b4ebe80978f32fd8b93f3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f55da0f6df604b5b9a45f4db14e4c45c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d435d0e7de948b38049a8f81aeaa34e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbfd63dcf99b49db8fa46029c01c403f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c55abdbad7834b6da983675fb8aebbd6","IPY_MODEL_7410f869f97f459a9a2313c9657b861e","IPY_MODEL_bad09f3098a740069e784aa6e9a53525"],"layout":"IPY_MODEL_97332d9dc4b74ba4893dc80cb1fcc9ac"}},"c55abdbad7834b6da983675fb8aebbd6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcdf12c77c4746338c44de308c92b3ef","placeholder":"​","style":"IPY_MODEL_9c4a5c97efa440b49484a3335e27153d","value":"tokenizer_config.json: 100%"}},"7410f869f97f459a9a2313c9657b861e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e5c409f0b714b2db9ee84741d3ec0a0","max":49,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d4eafa73cd85457ea38271e0ab918b54","value":49}},"bad09f3098a740069e784aa6e9a53525":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0ff1a9cc62549209e0f4aa24112dbab","placeholder":"​","style":"IPY_MODEL_7f1113e6082247e1b3ed02741b90e106","value":" 49.0/49.0 [00:00&lt;00:00, 1.76kB/s]"}},"97332d9dc4b74ba4893dc80cb1fcc9ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcdf12c77c4746338c44de308c92b3ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c4a5c97efa440b49484a3335e27153d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e5c409f0b714b2db9ee84741d3ec0a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4eafa73cd85457ea38271e0ab918b54":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d0ff1a9cc62549209e0f4aa24112dbab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f1113e6082247e1b3ed02741b90e106":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b316757d6324a5c84204d50d7e102a8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fad4d8ab520f422284a8911fa83ce5f4","IPY_MODEL_61cff305a0c64a08812fcff17c8766cc","IPY_MODEL_249ea8a92f244f0b9a1d1c16158fd404"],"layout":"IPY_MODEL_d09ba35311f3452780287cfc81c54d6c"}},"fad4d8ab520f422284a8911fa83ce5f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe51069de92c4ea69d94f8521be270d5","placeholder":"​","style":"IPY_MODEL_90f8a6d94b85438e9cf0755c4985e4a5","value":"vocab.txt: 100%"}},"61cff305a0c64a08812fcff17c8766cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_650567f8c1fc40eeba7f87e85f4259b3","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7e12cc2d100843ea9d299a9165506e64","value":213450}},"249ea8a92f244f0b9a1d1c16158fd404":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65a4605590d945e5a6cdd01b93d47e04","placeholder":"​","style":"IPY_MODEL_8d3dcffdea584f1586e542c8dbf766b6","value":" 213k/213k [00:00&lt;00:00, 1.08MB/s]"}},"d09ba35311f3452780287cfc81c54d6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe51069de92c4ea69d94f8521be270d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90f8a6d94b85438e9cf0755c4985e4a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"650567f8c1fc40eeba7f87e85f4259b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e12cc2d100843ea9d299a9165506e64":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65a4605590d945e5a6cdd01b93d47e04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d3dcffdea584f1586e542c8dbf766b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ca0d6c986b3483d82a903f63fda1da4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3b23e33859154d62951c1afc43b06976","IPY_MODEL_def06d6d582d4043bc9205dcafd6f9ef","IPY_MODEL_ca869ef3c2754039bae21f77f1172ffa"],"layout":"IPY_MODEL_06d158de67c64c7c977d524ce3f63c7a"}},"3b23e33859154d62951c1afc43b06976":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ba5809a07134feab04b6734105f5ff7","placeholder":"​","style":"IPY_MODEL_3db917c0e92945728a8fd4f380333cbe","value":"tokenizer.json: 100%"}},"def06d6d582d4043bc9205dcafd6f9ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_20a6fc1726d94b598aff47c34c310037","max":435797,"min":0,"orientation":"horizontal","style":"IPY_MODEL_74ce3a0c3cfb449d9b0c6adb686e2bce","value":435797}},"ca869ef3c2754039bae21f77f1172ffa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_701864790f0748d49bfb8543e42d10f9","placeholder":"​","style":"IPY_MODEL_6fda42a26e7b4cb995ddd1957f65e16b","value":" 436k/436k [00:00&lt;00:00, 3.34MB/s]"}},"06d158de67c64c7c977d524ce3f63c7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ba5809a07134feab04b6734105f5ff7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3db917c0e92945728a8fd4f380333cbe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20a6fc1726d94b598aff47c34c310037":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74ce3a0c3cfb449d9b0c6adb686e2bce":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"701864790f0748d49bfb8543e42d10f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fda42a26e7b4cb995ddd1957f65e16b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"T4X_qD-7LMy4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724812049983,"user_tz":-420,"elapsed":619,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"4e5eb08c-c76b-4e23-908d-b43d5192f7e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]}],"source":["!python --version"]},{"cell_type":"code","source":["# NLP Purpose\n","!pip install \"transformers[sentencepiece]\""],"metadata":{"id":"X7su527NLQQt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724812059849,"user_tz":-420,"elapsed":8390,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"a84897eb-52bf-49bd-b57d-ac2d43ae1d94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.23.5)\n","Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.5)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n","Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[sentencepiece]) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[sentencepiece]) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2024.7.4)\n"]}]},{"cell_type":"code","source":["import transformers\n","from transformers import pipeline\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"TP73SdXpLVAx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Transformers**\n","The **main features**:\n","- **Ease of use**: Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code.\n","- **Flexibility**: At their core, all models are simple PyTorch `nn.Module` or TensorFlow `tf.keras.Model` classes and can be handled like any other models in their respective machine learning (ML) frameworks.\n","- **Simplicity**: Hardly any abstractions are made across the library. The **“All in one file”** is a core concept: a model’s forward pass is entirely defined in a single file, so that the code itself is understandable and hackable.\n","\n","Discussing:\n","- Model API ➡ model and configuration model classes `pipeline()`\n","- Tokenizer API ➡ take care the first and last processing steps, handling the conversion from text to numerical, and the conversion back to text.\n","- Handle multiple sentences\n","- Look at the high-level `tokenizer()`"],"metadata":{"id":"revaJHAJLiy7"}},{"cell_type":"markdown","source":["## **Behind the Pipeline**\n"],"metadata":{"id":"EdomMm-ONEIL"}},{"cell_type":"code","source":["classifier = pipeline(\"sentiment-analysis\")\n","classifier(\n","    [\n","        \"I've been waiting for a HuggingFace course my whole life.\",\n","        \"I hate this so much!\",\n","    ]\n",")"],"metadata":{"id":"Z5azTwxZLjCH","colab":{"base_uri":"https://localhost:8080/","height":239,"referenced_widgets":["b23870baf9614bf999e25aaabe523a1f","c0f804b59a984f82849613a65f732d1f","ea898d791e85481c901ee570ab7e9976","d3667d214c7d4f8ca34d4fcc13408d9e","e68be92e273c40fb954031e6e0d8286a","b5b816fe8ad849b4a0e982582e554637","51db9f11949f42dfb8d9fae55ceaf4fc","c7b045a4fca94fe8bc02ba7ce59f00cd","40b287335ed64939a8bda2d395cca290","5288dcf4bd544ebeb2c49c43a6542cef","b3446108296349b2907f04de782dbc23","81ecf64ceeab4856a01621ee0ad92742","a1573852c60e48e8be33d62d7e2c7f04","ec0cfe8665794fb9ab16855aea9342ee","8d7dff107b7f48098bc3afe677ce1c9c","a89774abecd143bcbe64c947a868dc1b","f2234b17c0b4465793c6d83d83f605c5","252c8c6a280e4db781918ff69f35d7b4","cfbf7b96f84741c8b273d6637f5d4dd4","ef8779524a4c40078b40ee43add9f7a8","383a4d623c6b4b9e8685972deb65945d","13d88ece7234485187572109d794046b","d8e6715a66d846fb850a144e2d8a0577","352452c0cc3c4295af6054fe29a80096","fca62fe7a38e487eb7fd42c671453a6f","0fcf295adc594860a53b1a02b0e43d20","b9c0da288568464f9e33114871ddb044","a9793a89453941839086c6264d0df126","a728dc7404e24323adf443fbd1cb0d05","fbea12565ca84e8aa9053c1aa018eea3","a3992c5951d745dc97bbef3072dac790","83ad58b2cafb4ee3a7ee007e9662332a","3247374292f54cf091455ab75be96a99","07059310cb504155a485114fe04cfeba","f786e597afb44e7f94ff359c015fb87b","4bb7d9f2caf0458881e6f3995a02e292","43b8fef11b9f48829470cab9f9bf1958","4921c3e8937d4dafb9b9c0805ef9590a","55343ea83dc2425c9a85213fb1b34e2c","c4587e7c9aba4a2eacc2e36ac0caca6e","df6a02759db145f6b2f8b26125cc5216","b56831c9e9a5480ca5f43b69c862e024","595eaccab3c440b7853a10f465eb43d8","7ac79db3c7244adcadbd7cec54a53585"]},"executionInfo":{"status":"ok","timestamp":1724812112912,"user_tz":-420,"elapsed":8212,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"aff5e695-d15c-4ebf-dc06-cd28276dbe6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b23870baf9614bf999e25aaabe523a1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81ecf64ceeab4856a01621ee0ad92742"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8e6715a66d846fb850a144e2d8a0577"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07059310cb504155a485114fe04cfeba"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9598048329353333},\n"," {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["This pipeline groups together three steps: **preprocessing**, **passing the inputs through the model**, and **postprocessing**:=\n","\n","**TOKENIZER ➡ MODEL ➡ POST PROCESSING**\n","\n","[Raw Text ➡ input IDs ➡ Logits ➡ Prediction]"],"metadata":{"id":"5ln903MB0Uxl"}},{"cell_type":"markdown","source":["### **Preprocessing with a Tokenizer**\n","To convert the text inputs into numbers that the model can make sense of. To do this we use a *tokenizer*, which will be responsible for:\n","\n","- Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n","- Mapping each token to an integer\n","- Adding additional inputs that may be useful to the model\n","\n","All this preprocessing **needs to be done in exactly the same way as when the model was pretrained**.\n","\n","Using the ***checkpoint*** model and then preprocess with `AutoTokenizer` and it `from_pretained()`"],"metadata":{"id":"X84WRRh00lwt"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["776acfc657724bafb287b27ac8b7affc","4f0e9a20779f4e82a71f5f046b027138","3687de0a9bbe4570b56137be643fc3ae","0d0caf84270a4f25b4d86eeb0b66b308","6aa1b45360684ce09ad1800e2aea8f84","45fdc8968d0a46919332f1e8a062ae4f","a008593fd54e463b9eb5400661f2ed56","1cdf187344164236bc5aab7a4d9d6681","3895fdbae71f49518f7462335b7bc443","6325dc94a0b74aa48d0c4a64b25c7f4e","9e9dcc5c8fd84725a8972b68c84111d4","f918f55c0131404587e82d7babee9133","025d53962af3426f9b0cd0eeb18df93b","79132d53b43e425c82b4bb70fa9744de","8e0f666f6476481e8cf0b49654f04fa8","c6edc096ce8a4bae95dbd7f5cfbd1f8c","6c1516cb9dee46acb5d9dd84e98026b8","51cff89d57b64c2592f094ecf12f697c","78c13d358c4b47aaa66ba60f9d06fcee","a3b6d4a3024a436c88497553320dea59","cceb88d1e506484eac81906848a2a753","5c019a0b872a465f84d24a8cd99b11b5","34ea2fcaa08e47c4a8f67bcb1e9d361e","a8c88ba3342149dd9d50367e9a5f2523","ecc90b2ff1b84e9da9436d96c9e84ac6","048330877a454b748cb7c91c6ffcb9fa","98255685b6e746f0b4c94a638d789426","3d1292ae349642cf82f514b997500e8f","e988dee3aaf34b31ad2cf14be423b368","5d3980f83d444efa86ffa15995ee9ed2","0681b69843f540cfb0220e57b4e0b1f9","657446939ebc485686ea204efbaaf822","8954ef84ebbc4c7587781e1ca4f2e247"]},"id":"J9PURB_L1Wec","executionInfo":{"status":"ok","timestamp":1724812881849,"user_tz":-420,"elapsed":1499,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"8d32db47-dd03-4c2b-ab09-f65f0c845772"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"776acfc657724bafb287b27ac8b7affc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f918f55c0131404587e82d7babee9133"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34ea2fcaa08e47c4a8f67bcb1e9d361e"}},"metadata":{}}]},{"cell_type":"markdown","source":["Once we have the tokenizer, we can directly pass our sentences to it and we’ll get back a dictionary that’s ready to feed to our model!\n","\n","The only thing left to do is to convert the list of input IDs to tensors."],"metadata":{"id":"sQ1ntwiO187g"}},{"cell_type":"markdown","source":["**Transformer models only accept `tensors` as input.**\n","\n","To specify the type of tensors we want to get back, we use `return_tensors`"],"metadata":{"id":"AqTJhQ8p2LHn"}},{"cell_type":"code","source":["raw_inputs = [\n","    \"I've been waiting for a HuggingFace course my whole life.\",\n","    \"I hate this so much!\",\n","]\n","\n","# without return_tensors\n","inputs = tokenizer(raw_inputs, padding=True, truncation=True)\n","print(inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CikeWTRv2kAy","executionInfo":{"status":"ok","timestamp":1724812894839,"user_tz":-420,"elapsed":362,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"c4ac7786-c8fd-4732-cd42-bc5f0d8b3ad4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"]}]},{"cell_type":"code","source":["# with return tensors\n","inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n","print(inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FvlpoI7e23W1","executionInfo":{"status":"ok","timestamp":1724812906515,"user_tz":-420,"elapsed":404,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"a930bb1e-9c95-48cc-a4aa-87d476ca7e59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n","          2607,  2026,  2878,  2166,  1012,   102],\n","        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n","             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"]}]},{"cell_type":"markdown","source":["The output itself is a dictionary containing two keys, `input_ids` and `attention_mask`.\n","\n","`input_ids` contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence"],"metadata":{"id":"Iul99Jjm3YC2"}},{"cell_type":"markdown","source":["### **Going Through the Model**\n","We can download our pretrained model the same way we did with our tokenizer. 🤗 Transformers provides an `AutoModel` class which also has a `from_pretrained()` method:"],"metadata":{"id":"x7pyNgJy33Sl"}},{"cell_type":"code","source":["from transformers import AutoModel\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model = AutoModel.from_pretrained(checkpoint)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["7e1a01234276449ca4a9c80330ab1db8","eecc56f8d02240be9b400f13070bbf6d","abe97cd781024aff8d292ca4694501a4","d25966b7ebea440da4b66907b2ba80ea","fc096d831f7b4770b23f98bf89687f36","2c70c7f49c044621be91f6c418e1e40f","817b94a778ff42439f8bb6b8ee28d43e","ed853287d49a4d06af19c885f4506a6f","ca66bc7eed464fee97e63db991e80cf7","2c5ce98daa2b4c85a17df27dab7bee10","7564ceeb2d9e412d8d9c090575cfe09a"]},"id":"N0MaEN1y4BC7","executionInfo":{"status":"ok","timestamp":1724813547746,"user_tz":-420,"elapsed":7056,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"0000ef15-61e9-49e3-f620-d061588c2712"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e1a01234276449ca4a9c80330ab1db8"}},"metadata":{}}]},{"cell_type":"markdown","source":["➡ downloaded the same **checkpoint** we used in our **pipeline** before (it should actually have been cached already) and **instantiated a model with it**."],"metadata":{"id":"7DgP3srL4Iqw"}},{"cell_type":"markdown","source":["This architecture contains only the base Transformer module: given some inputs, it outputs what we’ll call *hidden states*, also known as *features*. For each model input, we’ll retrieve a high-dimensional vector representing the contextual understanding of that input by the Transformer model."],"metadata":{"id":"oopnNW4_4ih3"}},{"cell_type":"markdown","source":["#### **High-Dimensionality Vector**\n","The **vector output by the Transformer module** is usually large. It generally has three dimensions:\n","\n","- **Batch size**: The number of sequences processed at a time (2 in our example).\n","- **Sequence length**: The length of the numerical representation of the sequence (16 in our example).\n","- **Hidden size**: The vector dimension of each model input.\n","\n","It is said to be “high dimensional” because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more)."],"metadata":{"id":"_-AcrYEb461Q"}},{"cell_type":"code","source":["outputs = model(**inputs)\n","print(outputs.last_hidden_state.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRGdg18E4uhV","executionInfo":{"status":"ok","timestamp":1724813552618,"user_tz":-420,"elapsed":449,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"060b52ba-4dcc-400a-b614-28f333b73dbd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 16, 768])\n"]}]},{"cell_type":"markdown","source":["The outputs of Transformers model behave like `namedtuple` or *dictionaries* (Key and Value)"],"metadata":{"id":"HU5usTnT5p-E"}},{"cell_type":"code","source":["# output['last_hidden_state']"],"metadata":{"id":"qLSz6PqS57Lb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Model Heads: Making sense out of numbers** (*architecture*)\n","The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension.\n","\n","The **output of the Transformer model** is **sent directly to the model head to be processed**.\n","\n","The model represented by its embeddings layer and the subsequent layers. The embeddings layer converts each **input ID** in the tokenized input **into a vector that represents the associated token**. The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences."],"metadata":{"id":"uwOwPfw46PXQ"}},{"cell_type":"markdown","source":["There are *many different architectures* available in 🤗 Transformers, with each one designed around tackling a specific task.\n","\n","- `*Model` (retrieve the hidden states)\n","- `*ForCausalLM`\n","- `*ForMultipleChoice`\n","- `*ForQuestionAnswering`\n","- `*ForSequenceClassification`\n","- `*ForTokenClassification`\n","- and others 🤗"],"metadata":{"id":"vm_BC-nJ7C7p"}},{"cell_type":"markdown","source":["For our example, we will **need a model with a sequence classification head** (*to be able to classify the sentences as positive or negative*). So, we won’t actually use the `AutoModel` class, but `AutoModelForSequenceClassification`"],"metadata":{"id":"Be9FEbBR7dvf"}},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","outputs = model(**inputs)"],"metadata":{"id":"dnIL7pSt73eo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GO-FebhW785J","executionInfo":{"status":"ok","timestamp":1724814194100,"user_tz":-420,"elapsed":441,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"cd4a72b4-d937-4486-ab6b-4945a4e19d04"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n","        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["print(outputs.logits.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WHckblr978a9","executionInfo":{"status":"ok","timestamp":1724814203632,"user_tz":-420,"elapsed":426,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"d7be74bb-78bd-4c84-bff0-9de11f619fa3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 2])\n"]}]},{"cell_type":"markdown","source":["Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2."],"metadata":{"id":"F9sf68A-8DUD"}},{"cell_type":"markdown","source":["### **Post-processing the output**\n"],"metadata":{"id":"orWdJz4v8S_l"}},{"cell_type":"markdown","source":["The values we get as output from our model don’t necessarily make sense by themselves."],"metadata":{"id":"V3-uQlSb8Xhy"}},{"cell_type":"code","source":["print(outputs.logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_XJg1F58e3H","executionInfo":{"status":"ok","timestamp":1724814339421,"user_tz":-420,"elapsed":9,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"e2750890-5c25-4f05-d4bb-63520caae0b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-1.5607,  1.6123],\n","        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["Our model predicted `[-1.5607, 1.6123]` for the first sentence and `[ 4.1692, -3.3464]` for the second one. Those are not probabilities but *logits*, the raw, unnormalized scores outputted by the last layer of the model.\n","\n","To be converted to probabilities, they need to go through a **SoftMax** layer (*all Transformers models output the logits || loss-function for training SoftMax -- w/ actual loss-function Cross Entropy*)\n","\n"],"metadata":{"id":"-MVwYIvm8k1m"}},{"cell_type":"code","source":["import torch\n","\n","predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n","print(predictions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WXc2r6IL9lQu","executionInfo":{"status":"ok","timestamp":1724814633385,"user_tz":-420,"elapsed":380,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"7a66fcab-edcd-4252-8f05-2d1cec75a781"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[4.0195e-02, 9.5980e-01],\n","        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"markdown","source":["Now we can see that the model predicted `[0.0402, 0.9598]` for the first sentence and `[0.9995, 0.0005]` for the second one. These are recognizable probability scores."],"metadata":{"id":"Sd7bTClM9tts"}},{"cell_type":"markdown","source":["To get the labels corresponding to each position, we can inspect the `id2label` attribute of the model config."],"metadata":{"id":"RVrqYFra91WW"}},{"cell_type":"code","source":["model.config.id2label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R4zeoVyy9tIy","executionInfo":{"status":"ok","timestamp":1724814688564,"user_tz":-420,"elapsed":400,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"9b1ba7f4-7a26-4d9c-d484-201e90af317a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'NEGATIVE', 1: 'POSITIVE'}"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["Now we can conclude that the model predicted the following:\n","\n","- First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598\n","- Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005"],"metadata":{"id":"Y7n3kX6995hB"}},{"cell_type":"markdown","source":["## **Models**\n","Closer look at creating and using a model."],"metadata":{"id":"2VWoiTLx_bXQ"}},{"cell_type":"markdown","source":["The **`AutoModel`** class and all of its relatives are actually simple wrappers over the wide variety of models available in the library. It’s a clever wrapper as it **can automatically guess the appropriate model architecture for your checkpoint**, and then **instantiates a model with this architecture**.\n","\n","Let’s take a look at how this works with a **BERT** model."],"metadata":{"id":"QNWwxkh3_kRx"}},{"cell_type":"markdown","source":["### **Creating a Transformer**"],"metadata":{"id":"96UXMJ0NArVl"}},{"cell_type":"markdown","source":["The first thing we’ll need to do to initialize a BERT model is **load a configuration object**:"],"metadata":{"id":"fWP-OHnTAu1w"}},{"cell_type":"code","source":["from transformers import BertConfig, BertModel\n","\n","# Building the config\n","config = BertConfig()\n","\n","# Building the model from the config\n","model = BertModel(config) ## -- Model is randomly initialized"],"metadata":{"id":"alcgoKj8Ay--"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The **configuration** *contains* **many attributes** that are used to build the model:"],"metadata":{"id":"rB4wWdzeA2nC"}},{"cell_type":"code","source":["print(config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ESwnmpm-A1cq","executionInfo":{"status":"ok","timestamp":1724815501783,"user_tz":-420,"elapsed":422,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"d656b9a6-ee97-4ce2-a508-2af87d86480c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BertConfig {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.42.4\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n"]}]},{"cell_type":"markdown","source":["#### **Different Loading Methods**\n","Creating a model from the default configuration initializes it with random values:"],"metadata":{"id":"T2RmJvAuDdOz"}},{"cell_type":"code","source":["from transformers import BertConfig, BertModel\n","\n","config = BertConfig()\n","model = BertModel(config)\n","\n","## -- Model is randomly initialized"],"metadata":{"id":"z_J0bdLjD4IG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6sAvGfXLFeKJ","executionInfo":{"status":"ok","timestamp":1724816682591,"user_tz":-420,"elapsed":9,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"a0d89d0b-2e38-4c62-a280-b5d3479af4f1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSdpaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["The model can be used in this state, but it will output gibberish; it needs to be trained first. But this would require a long time and a lot of data, and it would have a non-negligible environmental impact.\n","\n","To avoid unnecessary and duplicated effort, it’s imperative to be able to **share and reuse models that have already been trained**."],"metadata":{"id":"qEUQAql_D7MX"}},{"cell_type":"code","source":["from transformers import BertModel\n","\n","# Load Transformer model that is already trained\n","model = BertModel.from_pretrained(\"bert-base-cased\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["d9b24b7d844b48348cf2393caeb1b820","528304f5f6fc47cbb879c3135cfeb937","3b404c31bb3f49088187f829f2f4f9da","9f24d72b9c0b48b19d1182828fb13be4","6bc375becc464451aedefeac988bbb4a","181e53fed398444d85effd2e0de0451b","5c95afbc46a3443e8b455673b069a1b9","b9274bd1b34b44b49e3ba296915bbba7","d526d3e0ce104bc6bd37e68311ed3716","f777b10106bd4a619dce84aec37571d0","4798624a302e41a19319710a3aea91e4","b55ba019a7c34b0388179470c139f985","578567ac5e8243c197da2c1b4c7e0339","4eb51034d83a4f989959cbfb4c63f1ac","27de2b1e787945bd804cb477016ce59e","595b0fdd153444e8975296aefb964258","6b9faa2692f94d69b604fe77cb24de34","5b6a920e3145467e81f443688d210948","3f63c388008d4a19b4bc282732af8c3d","3611ab10989b4ebe80978f32fd8b93f3","f55da0f6df604b5b9a45f4db14e4c45c","6d435d0e7de948b38049a8f81aeaa34e"]},"id":"df5tFwUTEM1z","executionInfo":{"status":"ok","timestamp":1724816715046,"user_tz":-420,"elapsed":7758,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"833ee700-65cf-4952-c58a-c7dbe388f5d6"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9b24b7d844b48348cf2393caeb1b820"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b55ba019a7c34b0388179470c139f985"}},"metadata":{}}]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_91EQlgPFj7J","executionInfo":{"status":"ok","timestamp":1724816722512,"user_tz":-420,"elapsed":428,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"2a64c8bc-7dfc-4338-8b72-ac05da21f820"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSdpaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["In the code sample above we didn’t use `BertConfig`, and instead loaded a pretrained model via the `bert-base-cased` identifier. This is a model checkpoint that was trained by the authors of **BERT** themselves."],"metadata":{"id":"jLGbxYQnE8BI"}},{"cell_type":"markdown","source":["This **`model`** is now *initialized with all the weights of the checkpoint*. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. **By training with pretrained weights rather than from scratch, we can quickly achieve good results**."],"metadata":{"id":"VpAXJCBLFH2U"}},{"cell_type":"markdown","source":["The identifier used to load the model can be the identifier of any model on the Model Hub, as long as it is compatible with the BERT architecture."],"metadata":{"id":"ukkxlzo-F_E3"}},{"cell_type":"markdown","source":["#### **Saving Methods**\n","Saving a model is as easy as loading one — we use the `save_pretrained()` method, which is analogous to the `from_pretrained()` method"],"metadata":{"id":"eD4MPeRVGBE0"}},{"cell_type":"code","source":["# model.save_pretrained(\"directory_on_my_computer\")\n","model.save_pretrained(\"config\")"],"metadata":{"id":"iDUtcv-aGI8X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Transformer model for Inference**\n","Transformer models **can only process numbers — numbers that the tokenizer generates**.\n","\n","But before we discuss tokenizers, let’s explore **what inputs the model accepts**."],"metadata":{"id":"LV_OI3SKHkeL"}},{"cell_type":"markdown","source":["Tokenizers can take care of casting the inputs to the appropriate framework’s tensors.\n","\n","Simple example of what tokenizers and tensors do:"],"metadata":{"id":"WGHvQRMSH8QX"}},{"cell_type":"code","source":["sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"],"metadata":{"id":"_22Md_L2H0Pu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The tokenizer converts these to vocabulary indices which are typically called input IDs. Each sequence is now a list of numbers! The resulting output is:"],"metadata":{"id":"TKrI30vFIAsC"}},{"cell_type":"code","source":["encoded_sequences = [\n","    [101, 7592, 999, 102],\n","    [101, 4658, 1012, 102],\n","    [101, 3835, 999, 102],\n","]"],"metadata":{"id":"3QzTqHmUH4Ga"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**This is a list of encoded sequences: a list of lists.**\n","\n","**Tensors** only accept *rectangular shapes* (think matrices). This “array” is already of rectangular shape, so converting it to a tensor is easy:"],"metadata":{"id":"f6EjxlnrIJA2"}},{"cell_type":"code","source":["import torch\n","\n","model_inputs = torch.tensor(encoded_sequences)"],"metadata":{"id":"7oWdBoWkIPOr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_inputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WCMLU_Z1ITfi","executionInfo":{"status":"ok","timestamp":1724817543742,"user_tz":-420,"elapsed":371,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"b0d33a50-7d0e-48f5-9f9a-82816d43b05e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 101, 7592,  999,  102],\n","        [ 101, 4658, 1012,  102],\n","        [ 101, 3835,  999,  102]])"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["**Using the tensors as inputs to the model**"],"metadata":{"id":"5kRJoY6RIX8B"}},{"cell_type":"code","source":["output = model(model_inputs)"],"metadata":{"id":"ZILyvq9XIXf4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YwRHmxa-Iakw","executionInfo":{"status":"ok","timestamp":1724817567279,"user_tz":-420,"elapsed":480,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"70262e0a-cb08-4827-b2f0-d8ea1c85bfce"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 4.4496e-01,  4.8276e-01,  2.7797e-01,  ..., -5.4032e-02,\n","           3.9393e-01, -9.4770e-02],\n","         [ 2.4943e-01, -4.4093e-01,  8.1772e-01,  ..., -3.1917e-01,\n","           2.2992e-01, -4.1172e-02],\n","         [ 1.3668e-01,  2.2518e-01,  1.4502e-01,  ..., -4.6915e-02,\n","           2.8224e-01,  7.5566e-02],\n","         [ 1.1789e+00,  1.6738e-01, -1.8187e-01,  ...,  2.4671e-01,\n","           1.0441e+00, -6.1972e-03]],\n","\n","        [[ 3.6436e-01,  3.2464e-02,  2.0258e-01,  ...,  6.0110e-02,\n","           3.2451e-01, -2.0996e-02],\n","         [ 7.1866e-01, -4.8725e-01,  5.1740e-01,  ..., -4.4012e-01,\n","           1.4553e-01, -3.7545e-02],\n","         [ 3.3223e-01, -2.3271e-01,  9.4876e-02,  ..., -2.5268e-01,\n","           3.2172e-01,  8.1085e-04],\n","         [ 1.2523e+00,  3.5754e-01, -5.1320e-02,  ..., -3.7840e-01,\n","           1.0526e+00, -5.6255e-01]],\n","\n","        [[ 2.4042e-01,  1.4718e-01,  1.2110e-01,  ...,  7.6062e-02,\n","           3.3564e-01,  2.8262e-01],\n","         [ 6.5701e-01, -3.2787e-01,  2.4968e-01,  ..., -2.5919e-01,\n","           2.0175e-01,  3.3275e-01],\n","         [ 2.0160e-01,  1.5783e-01,  9.8970e-03,  ..., -3.8850e-01,\n","           4.1307e-01,  3.9732e-01],\n","         [ 1.0175e+00,  6.4387e-01, -7.8147e-01,  ..., -4.2109e-01,\n","           1.0925e+00, -4.8456e-02]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.6856,  0.5262,  1.0000,  ...,  1.0000, -0.6112,  0.9971],\n","        [-0.6055,  0.4997,  0.9998,  ...,  0.9999, -0.6753,  0.9769],\n","        [-0.7702,  0.5447,  0.9999,  ...,  1.0000, -0.4655,  0.9894]],\n","       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["While the model accepts a lot of different arguments, only the input IDs are necessary."],"metadata":{"id":"m0fa-oXqKKaD"}},{"cell_type":"markdown","source":["## **Tokenizers**\n","**Tokenizers** are one of the core components of the NLP pipeline. They serve one purpose: **to translate text into data that can be processed by the model**. Models can only process numbers, so tokenizers need to **convert our text inputs to numerical data**.\n","\n","🥅 The goal is to find **the most meaningful representation** — that is, **the one that makes the most sense to the model** — and, if possible, **the smallest representation**."],"metadata":{"id":"9Nv96emSKax4"}},{"cell_type":"markdown","source":["### **Word-based**\n","The goal is to split the raw text into words and find a numerical representation for each of them"],"metadata":{"id":"lvveQEalLxRK"}},{"cell_type":"markdown","source":["- `Split()` the sentences"],"metadata":{"id":"3l7cOJZMMQm5"}},{"cell_type":"code","source":["tokenized_text = \"Jim Henson was a puppeteer\".split()\n","print(tokenized_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U2RHOTT0MPLo","executionInfo":{"status":"ok","timestamp":1724818488437,"user_tz":-420,"elapsed":381,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"a0edcd53-8a0e-4e80-f774-936ad7d52497"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"]}]},{"cell_type":"markdown","source":[". With this kind of tokenizer, we can end up with some pretty large *“vocabularies*,” where a **vocabulary** is **defined by the total number of independent tokens that we have in our corpus.**\n","\n","Each word gets assigned an **ID**, *starting from 0 and going up to the size of the vocabulary*. **The model uses these IDs to identify each word**."],"metadata":{"id":"3Rjbq_P8MYH7"}},{"cell_type":"markdown","source":["Finally, we need a **custom token** to represent words that are not in our vocabulary. This is known as the *“unknown”* token, often represented as ”`[UNK]`” or ”`<unk>`”. It wasn’t able to retrieve a sensible representation of a word and you’re losing information along the way."],"metadata":{"id":"jrLySJZEM3Gn"}},{"cell_type":"markdown","source":["### **Charracter-based**\n","**Reduce the amount of unknown tokens**.\n","\n","➡ **Character-based** tokenizers **split the text into characters, rather than words**."],"metadata":{"id":"PGpPNAcbNTNn"}},{"cell_type":"markdown","source":["This approach isn’t perfect either. Since the representation is now based on characters rather than words, one could argue that, intuitively, it’s less meaningful: **each character doesn’t mean a lot on its own, whereas that is the case with words**."],"metadata":{"id":"GM6Mx1hSNmLk"}},{"cell_type":"markdown","source":["Another thing to consider is that we’ll end up with a very large amount of tokens to be processed by our model: whereas a word would only be a single token with a word-based tokenizer, it can easily turn into 10 or more tokens when converted into characters."],"metadata":{"id":"sFNKON7TNupb"}},{"cell_type":"markdown","source":["### **Subword tokenization**\n","**Get the best of both worlds** (word - character).\n","\n","➡ Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords."],"metadata":{"id":"mH7MEqFwNxPY"}},{"cell_type":"markdown","source":["For instance, “annoyingly” might be considered a rare word and could be decomposed into “annoying” and “ly”. These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of “annoyingly” is kept by the composite meaning of “annoying” and “ly”."],"metadata":{"id":"l4jhSAqeOln7"}},{"cell_type":"markdown","source":["### **Loading and Saving**\n","Loading and saving tokenizers is as simple as it is with models. Actually, it’s based on the same two methods: `from_pretrained()` and `save_pretrained()`. These methods will load or save the algorithm used by the tokenizer (a bit like the *architecture* of the model) as well as its *vocabulary* (a bit like the weights of the model)."],"metadata":{"id":"G7tI45XuOse1"}},{"cell_type":"markdown","source":["Loading the **BERT** tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the BertTokenizer class"],"metadata":{"id":"fPcTMA5ka8uO"}},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["cbfd63dcf99b49db8fa46029c01c403f","c55abdbad7834b6da983675fb8aebbd6","7410f869f97f459a9a2313c9657b861e","bad09f3098a740069e784aa6e9a53525","97332d9dc4b74ba4893dc80cb1fcc9ac","bcdf12c77c4746338c44de308c92b3ef","9c4a5c97efa440b49484a3335e27153d","7e5c409f0b714b2db9ee84741d3ec0a0","d4eafa73cd85457ea38271e0ab918b54","d0ff1a9cc62549209e0f4aa24112dbab","7f1113e6082247e1b3ed02741b90e106","9b316757d6324a5c84204d50d7e102a8","fad4d8ab520f422284a8911fa83ce5f4","61cff305a0c64a08812fcff17c8766cc","249ea8a92f244f0b9a1d1c16158fd404","d09ba35311f3452780287cfc81c54d6c","fe51069de92c4ea69d94f8521be270d5","90f8a6d94b85438e9cf0755c4985e4a5","650567f8c1fc40eeba7f87e85f4259b3","7e12cc2d100843ea9d299a9165506e64","65a4605590d945e5a6cdd01b93d47e04","8d3dcffdea584f1586e542c8dbf766b6","6ca0d6c986b3483d82a903f63fda1da4","3b23e33859154d62951c1afc43b06976","def06d6d582d4043bc9205dcafd6f9ef","ca869ef3c2754039bae21f77f1172ffa","06d158de67c64c7c977d524ce3f63c7a","8ba5809a07134feab04b6734105f5ff7","3db917c0e92945728a8fd4f380333cbe","20a6fc1726d94b598aff47c34c310037","74ce3a0c3cfb449d9b0c6adb686e2bce","701864790f0748d49bfb8543e42d10f9","6fda42a26e7b4cb995ddd1957f65e16b"]},"id":"t33lSroca_8E","executionInfo":{"status":"ok","timestamp":1724822388965,"user_tz":-420,"elapsed":1645,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"c1e663e8-6452-46fe-865e-5d4ce5b0f51f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbfd63dcf99b49db8fa46029c01c403f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b316757d6324a5c84204d50d7e102a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ca0d6c986b3483d82a903f63fda1da4"}},"metadata":{}}]},{"cell_type":"markdown","source":["Similar to `AutoModel`, the `AutoTokenizer` class will grab the proper **tokenizer** class in the library based on the checkpoint name, and can be used directly with any checkpoint:"],"metadata":{"id":"govJy2ysbP-H"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"],"metadata":{"id":"MN9XlvLxbQ1T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer(\"Using a Transformer network is simple\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EmmuaDj2bWtt","executionInfo":{"status":"ok","timestamp":1724822432671,"user_tz":-420,"elapsed":459,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"3d8f35ab-1643-41bf-b64a-8c42e71d3184"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","source":["saving a tokenizer"],"metadata":{"id":"nEk3LK2Ybivm"}},{"cell_type":"code","source":["# tokenizer.save_pretrained(\"directory_on_my_computer\")\n","tokenizer.save_pretrained(\"config\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A2TfJFw3biAm","executionInfo":{"status":"ok","timestamp":1724822501847,"user_tz":-420,"elapsed":379,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"d2b2ed0c-4735-430e-e254-94f54c624204"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('config/tokenizer_config.json',\n"," 'config/special_tokens_map.json',\n"," 'config/vocab.txt',\n"," 'config/added_tokens.json',\n"," 'config/tokenizer.json')"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["First, let’s see how the `input_ids` are generated. To do this, we’ll need to look at the intermediate methods of the tokenizer."],"metadata":{"id":"r2pFx_X-cEkS"}},{"cell_type":"markdown","source":["### **Encoding**\n","Translating text to numbers is known as *encoding*. Encoding is done in a two-step process: the **tokenization**, followed by the **conversion to input IDs**.\n","\n","**Steps**:\n","1. **Split text into words** (*tokens*) ➡ [*which is why we need to **instantiate the tokenizer using the name of the model**, to make sure we **use the same rules** that were used when the model was **pretrained**.*]\n","2. Convert **tokens into numbers** ➡ [*.**build a `tensor` out of them and feed them to the model**. To do this, the tokenizer has a **vocabulary**, which is the part we download when we instantiate it with the `from_pretrained()` method.*]"],"metadata":{"id":"rbZ9g90HcGvt"}},{"cell_type":"markdown","source":["#### **Tokenization**\n","The tokenization process is done by the `tokenize()` method of the tokenizer"],"metadata":{"id":"c6HmIX48ebua"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","sequence = [\n","            \"I’ve been waiting for a HuggingFace course my whole life.\",\n","            \"I hate this so much!\",\n","        ]\n","tokens = tokenizer.tokenize(sequence)\n","\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mtsHX_Toefpk","executionInfo":{"status":"ok","timestamp":1724824181702,"user_tz":-420,"elapsed":417,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"847e31e6-45aa-4fee-c596-a3909a6d0828"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', '’', 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.', 'i', 'hate', 'this', 'so', 'much', '!']\n"]}]},{"cell_type":"markdown","source":["This tokenizer is a subword tokenizer: it splits the words until it obtains tokens that can be represented by its vocabulary"],"metadata":{"id":"eLHWgZuLe4ZN"}},{"cell_type":"markdown","source":["#### **Tokens to Input IDs**\n","The conversion to input IDs is handled by the `convert_tokens_to_ids()` tokenizer method\n"],"metadata":{"id":"7_k9sTAge6nA"}},{"cell_type":"code","source":["ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SGXDiwtifGRA","executionInfo":{"status":"ok","timestamp":1724824184211,"user_tz":-420,"elapsed":499,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"60189d1d-eff8-42e8-9b1e-8d8f2a80cb56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1045, 1521, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 1045, 5223, 2023, 2061, 2172, 999]\n"]}]},{"cell_type":"markdown","source":["### **Decoding**\n","**From vocabulary indices**, we want **to get a string**. This can be done with the `decode()` method"],"metadata":{"id":"MzGLzKv0sbGO"}},{"cell_type":"code","source":["decoded_string = tokenizer.decode([1045, 5223, 2023, 2061, 2172, 999])\n","\n","print(decoded_string)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W3pcTl94sjTo","executionInfo":{"status":"ok","timestamp":1724826974600,"user_tz":-420,"elapsed":405,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"ca6b98dc-780f-48bc-ea0b-10990554fcd0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["i hate this so much!\n"]}]},{"cell_type":"markdown","source":["Note that the decode method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence"],"metadata":{"id":"mg8_GTWUs5R2"}},{"cell_type":"markdown","source":["## **Handling Multiple Sequences**\n","Some questions emerge:\n","- How do we **handle multiple sequences**?\n","- How do we handle multiple sequences of **different lengths**?\n","- Are **vocabulary indices** the only inputs that allow a model to work well?\n","- Is there such a thing as **too long a sequence**?"],"metadata":{"id":"21j8bsAks9c8"}},{"cell_type":"markdown","source":["### **Models expect a batch on inputs**\n","Let’s convert this list of numbers to a tensor and send it to the models"],"metadata":{"id":"eFuTm_hKtVBf"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","## Same checkpoint for Tokenizer and Model\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","input_ids = torch.tensor(ids) # the problem is here\n","\n","# This line will fail\n","model(input_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":375},"id":"1WkgLlgitd_X","executionInfo":{"status":"error","timestamp":1724827357912,"user_tz":-420,"elapsed":3280,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"77da5858-d2f9-4082-82ea-ef20475b9419"},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"too many indices for tensor of dimension 1","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-fa3ef026e594>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# This line will fail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m         distilbert_output = self.distilbert(\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn_if_padding_and_no_attention_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mwarn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m   4539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4540\u001b[0m         \u001b[0;31m# Check only the first and last input IDs to reduce overhead.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4541\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4542\u001b[0m             warn_string = (\n\u001b[1;32m   4543\u001b[0m                 \u001b[0;34m\"We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"]}]},{"cell_type":"markdown","source":["**FAIL**.\n","\n","The problem is that we sent a single sequence to the model, whereas 🤗 **Transformers models expect multiple sentences by default**.\n","\n","Here we tried to do everything the tokenizer did behind the scenes when we applied it to a `sequence`. But if you look closely, you’ll see that the tokenizer didn’t just convert the list of input IDs into a tensor, it added a *dimension* on top of it"],"metadata":{"id":"j0iIXAqStzO_"}},{"cell_type":"code","source":["tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n","print(tokenized_inputs[\"input_ids\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9aV9wtaKuIEt","executionInfo":{"status":"ok","timestamp":1724827367729,"user_tz":-420,"elapsed":383,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"cdeb9c78-03bf-4cb9-aa9e-805e2f33a4ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n","          2607,  2026,  2878,  2166,  1012,   102]])\n"]}]},{"cell_type":"markdown","source":["Try again and add a new dimension:"],"metadata":{"id":"1owAXl5suaAw"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","input_ids = torch.tensor([ids]) # <- here's the different\n","print(\"Input IDs:\", input_ids)\n","\n","output = model(input_ids)\n","print(\"Logits:\", output.logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QGZT8AykucHl","executionInfo":{"status":"ok","timestamp":1724827494257,"user_tz":-420,"elapsed":894,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"4424440a-5d4f-495b-ad09-ddee9abac3f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n","          2026,  2878,  2166,  1012]])\n","Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["**`Batching`** is the act of **sending multiple sentences through the model, all at once**.\n","\n","If you only have **one sentence**, you can just build a batch with a single sequence (***two identical batches***):"],"metadata":{"id":"q4bKioW7u0eY"}},{"cell_type":"code","source":["batched_ids = [ids, ids]"],"metadata":{"id":"d4ek47C9u4-W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Batches into tensor\n","batch_input_ids = torch.tensor(batched_ids) # <-- here's the different\n","print(\"Input IDs:\", batch_input_ids)\n","\n","batch_output = model(batch_input_ids)\n","print(\"Logits:\", batch_output.logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x8vvJbZZvCPM","executionInfo":{"status":"ok","timestamp":1724827676115,"user_tz":-420,"elapsed":399,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"ad465eaf-578f-47c5-e140-ec50505a2f5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n","          2026,  2878,  2166,  1012],\n","        [ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n","          2026,  2878,  2166,  1012]])\n","Logits: tensor([[-2.7276,  2.8789],\n","        [-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["### **Padding the Inputs**\n","Solve the different lengths of Batch or sentences.\n","\n","🥅 **Having the same length by using *padding token*.**\n","\n","The padding token ID can be found in `tokenizer.pad_token_id`"],"metadata":{"id":"Fi2TkMANv5qe"}},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequence1_ids = [[200, 200, 200]]\n","sequence2_ids = [[200, 200]]\n","batched_ids = [\n","    [200, 200, 200],\n","    [200, 200, tokenizer.pad_token_id],\n","]\n","\n","print(model(torch.tensor(sequence1_ids)).logits)\n","print(model(torch.tensor(sequence2_ids)).logits)\n","print(model(torch.tensor(batched_ids)).logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gTJkwPmuwaFb","executionInfo":{"status":"ok","timestamp":1724827969323,"user_tz":-420,"elapsed":1400,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"a2252fac-7e5a-4eb5-eccf-3a48fec2b654"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n","tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n","tensor([[ 1.5694, -1.3895],\n","        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["There’s something wrong with the logits in our batched predictions: *the second row should be the same as the logits for the second sentence*, but we’ve got completely different values!\n","\n","This is because the key feature of Transformer models is attention layers that *contextualize* each token.\n","\n","✅ To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to **tell those attention layers to ignore the padding tokens**. This is done by using an `attention mask`."],"metadata":{"id":"_81xUQ0ZwlGX"}},{"cell_type":"markdown","source":["### **Attention Masks**\n","*Attention masks* are tensors with the **exact same shape as the input IDs tensor**, filled with 0s and 1s: ***1s indicate the corresponding tokens should be attended to***, and ***0s indicate the corresponding tokens should not be attended to*** (i.e., they should be ignored by the attention layers of the model)."],"metadata":{"id":"TqecmSoNxdbC"}},{"cell_type":"code","source":["batched_ids = [\n","    [200, 200, 200],\n","    [200, 200, tokenizer.pad_token_id],\n","]\n","\n","attention_mask = [\n","    [1, 1, 1],\n","    [1, 1, 0],\n","]\n","\n","outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n","print(outputs.logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4HWgPQHix5YY","executionInfo":{"status":"ok","timestamp":1724828383810,"user_tz":-420,"elapsed":415,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"bbcecd7f-1da3-4e81-e404-5d2ee245981b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.5694, -1.3895],\n","        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["Now we get the same logits for the second sentence in the batch."],"metadata":{"id":"Lq4K9h1ry5ms"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModel\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","padd = tokenizer.pad_token_id\n","\n","batched_sequence = [\n","    [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],\n","    [  101,  1045,  5223,  2023,  2061,  2172,   999,   102, padd, padd, padd, padd, padd, padd, padd, padd],\n","]\n","\n","attention_mask_sequence = [\n","    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","    [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","]\n","\n","output_sequence = model(torch.tensor(batched_sequence), attention_mask=torch.tensor(attention_mask_sequence))\n","print(output_sequence.logits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9xEGgcH-y5Xa","executionInfo":{"status":"ok","timestamp":1724829260698,"user_tz":-420,"elapsed":1009,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"9589699b-8a61-44c8-8e00-4204184cf285"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-1.5607,  1.6123],\n","        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["### **Longer Sequences**\n","With Transformer models, there is a **limit to the lengths of the sequences we can pass the models**. Most models handle sequences of up to `512 or 1024 tokens`, and **will crash when asked to process longer sequences**.\n","\n","**Two Solutions:**\n","- Use a model with a longer supported sequence length.\n","- Truncate your sequences.\n","\n","Otherwise, we recommend you **truncate** your sequences by specifying the `max_sequence_length` parameter:"],"metadata":{"id":"_m0SX7O50VF-"}},{"cell_type":"code","source":["# Get from the maximum length inside list of text/documents\n","max_sequence_length = 0\n","\n","sequence = sequence[:max_sequence_length]"],"metadata":{"id":"J_8DoJhr2t_W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Putting it all Together**\n","By Transformers API, when you call your **`tokenizer`** directly on the sentence, you get back inputs that are ready to pass through your model:"],"metadata":{"id":"JELt5FQB3GDc"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","model_inputs = tokenizer(sequence)"],"metadata":{"id":"QPfN3xiD3UNy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_inputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BEcWeTdu3ZIK","executionInfo":{"status":"ok","timestamp":1724829782172,"user_tz":-420,"elapsed":412,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"f0763bdd-9d4a-4298-da5e-6cbe9aa8d7e3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"metadata":{},"execution_count":84}]},{"cell_type":"markdown","source":["Here, the `model_inputs` variable contains everything that’s necessary for a model to operate well.\n","\n","For **DistilBERT**, that includes the input IDs as well as the attention mask. Other models that accept additional inputs will also have those output by the `tokenizer` object."],"metadata":{"id":"GInc-1IM3YZ8"}},{"cell_type":"markdown","source":["**The powerful of `tokenizer`:**"],"metadata":{"id":"egbm62Yg3t-y"}},{"cell_type":"markdown","source":["First, it can `tokenize` single sequence"],"metadata":{"id":"EO3Vp6bT3xgA"}},{"cell_type":"code","source":["sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","model_inputs = tokenizer(sequence)"],"metadata":{"id":"pFd3zOfX350C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_inputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pbzMFx5O41h7","executionInfo":{"status":"ok","timestamp":1724830152637,"user_tz":-420,"elapsed":29,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"b9904e76-2fe0-43f2-b1a5-80623489b5a4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"metadata":{},"execution_count":86}]},{"cell_type":"markdown","source":["Second, it can `tokenize` multiple sequence at a time"],"metadata":{"id":"GFtSUddv30vA"}},{"cell_type":"code","source":["sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","model_inputs = tokenizer(sequences)"],"metadata":{"id":"Oy3Qehn837a9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_inputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gFxluLAW43wF","executionInfo":{"status":"ok","timestamp":1724830157417,"user_tz":-420,"elapsed":10,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"67586b1f-7ae1-4487-ccde-2b97e8e20279"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"]},"metadata":{},"execution_count":88}]},{"cell_type":"markdown","source":["Third, it can `pad` according to several objectives"],"metadata":{"id":"EnN7PJJB31Kv"}},{"cell_type":"code","source":["# Will pad the sequences up to the maximum sequence length\n","model_inputs = tokenizer(sequences, padding=\"longest\")\n","\n","# Will pad the sequences up to the model max length\n","# (512 for BERT or DistilBERT)\n","model_inputs = tokenizer(sequences, padding=\"max_length\")\n","\n","# Will pad the sequences up to the specified max length\n","model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"],"metadata":{"id":"9_qkerMu4AMG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_inputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Crl_ZtP346ey","executionInfo":{"status":"ok","timestamp":1724830171216,"user_tz":-420,"elapsed":458,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"6b5199e7-f9db-459f-9f5d-265928fae810"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0]]}"]},"metadata":{},"execution_count":90}]},{"cell_type":"markdown","source":["Fourth, it can also `truncate` sequences"],"metadata":{"id":"05vjTWlX4JVl"}},{"cell_type":"code","source":["sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","# Will truncate the sequences that are longer than the model max length\n","# (512 for BERT or DistilBERT)\n","model_inputs = tokenizer(sequences, truncation=True)\n","\n","# Will truncate the sequences that are longer than the specified max length\n","model_inputs = tokenizer(sequences, max_length=8, truncation=True)"],"metadata":{"id":"elAwBjsx4O--"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_inputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPrsu75M5Ec-","executionInfo":{"status":"ok","timestamp":1724830212640,"user_tz":-420,"elapsed":373,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"d898ce60-549e-4219-c71f-890a441d6a5c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"]},"metadata":{},"execution_count":92}]},{"cell_type":"markdown","source":["The `tokenizer` object can handle the conversion to specific framework tensors, which can then be directly sent to the model.\n","- PyTorch ➡ `\"pt\"`\n","- TensorFlow ➡ `\"tf\"`\n","- NumPy arrays ➡ `\"np\"`"],"metadata":{"id":"6rGIhW8Y4dY0"}},{"cell_type":"code","source":["sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","# Returns PyTorch tensors\n","model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n","\n","# Returns TensorFlow tensors\n","model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n","\n","# Returns NumPy arrays\n","model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"],"metadata":{"id":"Da5Aa4t24o_6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_inputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SV95FVGW5Jny","executionInfo":{"status":"ok","timestamp":1724830235088,"user_tz":-420,"elapsed":473,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"8fb953df-58c3-4ef1-aae6-0831227185e5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n","        12172,  2607,  2026,  2878,  2166,  1012,   102],\n","       [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"]},"metadata":{},"execution_count":94}]},{"cell_type":"markdown","source":["### **Special Tokens**\n"],"metadata":{"id":"o_FoH9vJ4ss1"}},{"cell_type":"code","source":["sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n","\n","model_inputs = tokenizer(sequence)\n","print(model_inputs[\"input_ids\"])\n","\n","tokens = tokenizer.tokenize(sequence)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z4J_rtrr4xVe","executionInfo":{"status":"ok","timestamp":1724830249765,"user_tz":-420,"elapsed":403,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"9e6166f3-60c7-4957-a16f-fee8dd64d8b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n","[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"]}]},{"cell_type":"markdown","source":["One token ID was added at the beginning, and one at the end"],"metadata":{"id":"u8QxpgGz5dyT"}},{"cell_type":"code","source":["print(tokenizer.decode(model_inputs[\"input_ids\"]))\n","print(tokenizer.decode(ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I2DJKZV_5eNm","executionInfo":{"status":"ok","timestamp":1724830314927,"user_tz":-420,"elapsed":416,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"e76e236e-0996-41d1-aa0e-3199a468ec8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n","i've been waiting for a huggingface course my whole life.\n"]}]},{"cell_type":"markdown","source":["The `tokenizer` added the special word `[CLS]` at the beginning and the special word `[SEP]` at the end.\n","\n","This is because the model was pretrained with those, so to get the same results for inference we need to add them as well.\n","\n","Note that some models don’t add special words, or add different ones; models may also add these special words only at the beginning, or only at the end.\n","\n","🎯 **In any case, the `tokenizer` knows which ones are expected and will deal with this for you.**"],"metadata":{"id":"Yiu5IsCv5pBi"}},{"cell_type":"markdown","source":["### **Wrapping Up: Tokenizer to Model**\n","let’s see one final time how it can handle multiple sequences (padding!), very long sequences (truncation!), and multiple types of tensors with its main API"],"metadata":{"id":"N_r_5huG55V2"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","\n","sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n","\n","tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n","output = model(**tokens)"],"metadata":{"id":"tCVLJWNz6CbZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output.logits"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qPiNEsnd6KOp","executionInfo":{"status":"ok","timestamp":1724830503106,"user_tz":-420,"elapsed":10,"user":{"displayName":"Aditya Hermawan","userId":"03388716886774187798"}},"outputId":"3c1013b1-cdcd-4283-8aff-1472196928a3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-1.5607,  1.6123],\n","        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":99}]}]}