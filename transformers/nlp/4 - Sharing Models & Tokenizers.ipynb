{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eio9CCgNDox9"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP Purpose\n",
        "!pip install \"transformers[sentencepiece]\" datasets"
      ],
      "metadata": {
        "id": "lL_BAbPiQ_hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using Pretrained Models**"
      ],
      "metadata": {
        "id": "Zkv_XSUQEmVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs say we‚Äôre looking for a French-based model that can perform mask filling.\n",
        "\n",
        "We select the `camembert-base` checkpoint to try it out. The identifier `camembert-base` is all we need to start using it!"
      ],
      "metadata": {
        "id": "F-uH-iFUZ3oD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "camembert_fill_mask = pipeline(\"fill-mask\", model=\"camembert-base\")\n",
        "results = camembert_fill_mask(\"Le camembert est <mask> :)\")"
      ],
      "metadata": {
        "id": "J24ULP8KaBW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, loading a model within a pipeline is extremely simple. The only thing you need to watch out for is that the **chosen checkpoint is suitable for the task it‚Äôs going to be used for**.\n",
        "\n",
        "We recommend using the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints:"
      ],
      "metadata": {
        "id": "1LItq4ryaKlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also instantiate the checkpoint using the model architecture directly:"
      ],
      "metadata": {
        "id": "enHjUsHZaiPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CamembertTokenizer, CamembertForMaskedLM\n",
        "\n",
        "tokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\n",
        "model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")"
      ],
      "metadata": {
        "id": "h2RhqTWfaZPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we recommend using the `Auto\\* classes` instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the `Auto\\* classes` makes switching checkpoints simple"
      ],
      "metadata": {
        "id": "zJPxvw2hanVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"camembert-base\")"
      ],
      "metadata": {
        "id": "pjYdcRymaxbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using a pretrained model, make sure to **check how it was trained, on which datasets, its limits, and its biases.**"
      ],
      "metadata": {
        "id": "hAhSxztidDoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sharing Pretrained Models**\n",
        "There are three ways to go about creating new model repositories:\n",
        "\n",
        "- Using the `push_to_hub` API\n",
        "- Using the `huggingface_hub` Python library\n",
        "- Using the `web interface`\n",
        "\n",
        "Once you‚Äôve created a repository, you can upload files to it via git and git-lfs."
      ],
      "metadata": {
        "id": "YclrdczaEofl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Using `push_to_hub` API**"
      ],
      "metadata": {
        "id": "a7xeZwJcHrEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Generate an Authentication** (*username* and *password*)"
      ],
      "metadata": {
        "id": "2U_OGOleJYv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n",
        "\n",
        "# --- terminal\n",
        "# huggingface-cli login"
      ],
      "metadata": {
        "id": "9qobvvEzJmko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. If using the `Trainer`, we set `push_to_hub=True` inside `TrainingArguments`"
      ],
      "metadata": {
        "id": "Efx3h5FxJytr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    # hub_model_id = \"differenet_name\",\n",
        "    \"bert-finetuned-mrpc\",\n",
        "    save_strategy=\"epoch\",\n",
        "    push_to_hub=True      # set to True\n",
        ")"
      ],
      "metadata": {
        "id": "f1KjlY6cJ-dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you call `trainer.train()`, the `Trainer` will then upload your model to the Hub each time it is saved (here every epoch) in a repository in your namespace.\n",
        "\n",
        "That repository will be named like the output directory you picked (here `bert-finetuned-mrpc`) but you can choose a different name with **`hub_model_id = \"a_different_name\"`.**"
      ],
      "metadata": {
        "id": "VEbA4RHoKQKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once your training is finished, you should do a final"
      ],
      "metadata": {
        "id": "M9gWiDJYKkkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After training finished, run this\n",
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "i-1D8k4zKoAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the code above is to **upload the last version of the model**. It will also generate a model card with all the relevant metadata, reporting the hyperparameters used and the evaluation results!"
      ],
      "metadata": {
        "id": "9AHnoWnwKouU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 If not using the `Trainer` we can push directly on models, tokenizers, and configurations via `push_to_hub()` also"
      ],
      "metadata": {
        "id": "0uOioaW4NLFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "checkpoint = \"camembert-base\"\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "asU09wBxNZoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do whatever we want with these (add token, change labels, fine-tune, etc.)"
      ],
      "metadata": {
        "id": "tJwfwLpdNcv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_name = \"repo_location\"\n",
        "\n",
        "model.push_to_hub(repo_name)\n",
        "tokenizers.push_to_hub(repo_name)\n",
        "# tokenizers.push_to_hub(repo_name, organization=\"\")\n",
        "# tokenizers.push_to_hub(repo_name, organization=\"\", use_auth_token=\"<TOKEN>\")"
      ],
      "metadata": {
        "id": "PYAtoOMfNi1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Using the `huggingface_hub` Python lobrary**\n",
        "The `huggingface_hub` Python library is a package which offers a set of tools for the model and datasets hubs.\n",
        "- **It provides simple methods and classes for common tasks** (information about repositories on the hub and managing them).\n",
        "- **It provides simple APIs** that work on top of git to manage those repositories‚Äô content and to integrate the Hub in your projects and libraries."
      ],
      "metadata": {
        "id": "rHATmGE3OEUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Authenticate**"
      ],
      "metadata": {
        "id": "bSPXeZNWOKev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface-cli login\n",
        "\n",
        "## if running in Google Colab\n",
        "# !huggingface-cli login"
      ],
      "metadata": {
        "id": "kp5xM7GuOKw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Methods to manage *a local* repository**"
      ],
      "metadata": {
        "id": "unfUHrHOPOgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import (\n",
        "    # User management\n",
        "    login,\n",
        "    logout,\n",
        "    whoami,\n",
        "\n",
        "    # Repository creation and management\n",
        "    create_repo,\n",
        "    delete_repo,\n",
        "    update_repo_visibility,\n",
        "\n",
        "    # And some methods to retrieve/change information about the content\n",
        "    list_models,\n",
        "    list_datasets,\n",
        "    list_metrics,\n",
        "    list_repo_files,\n",
        "    upload_file,\n",
        "    delete_file,\n",
        ")"
      ],
      "metadata": {
        "id": "_xVJsbp-PRq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---- `create_repo` ‚û° create a new repository on the hub"
      ],
      "metadata": {
        "id": "ojMvFmwzPeVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "create_repo(\"dummy-model\") # create repo 'dummy-model' in namespace\n",
        "create_repo(\"dummy-model\", organization=\"\") # create repo that belong to the organization\n",
        "\n",
        "# Other arugments\n",
        "## -- private ‚û° visibility of the repository\n",
        "## -- token ‚û° override the token stored\n",
        "## -- repo_type ‚û° instead create a model -- 'dataset' or 'space'"
      ],
      "metadata": {
        "id": "EHJJ1SkgPnNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Using the Web Interface**\n",
        "The web interface offers tools to manage repositories directly in the Hub. Using the interface, you can easily create repositories, add files (even large ones!), explore models, visualize diffs, and much more"
      ],
      "metadata": {
        "id": "6ndVPX9RQS5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Uploading the Model files**\n",
        "The system to manage files on the Hugging Face Hub is based on **`git` for regular files**, and **`git-lfs**` (which stands for Git Large File Storage) **for larger files**."
      ],
      "metadata": {
        "id": "_8cdS5NjQ40Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. The `upload_file` approach**\n",
        "Does not require git or git-lfs, it **pushes file directly to the Hub**.\n",
        "\n",
        "A limitation of this approach is, it **doesn't handle larger than 5GB in size**."
      ],
      "metadata": {
        "id": "sleh9b_bRTX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the files are larger than 5GB then,"
      ],
      "metadata": {
        "id": "7J5GQ2kHRtC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "upload_file(\n",
        "    \"<path_to_file>/config.json\",\n",
        "    path_in_repo=\"config.json\",\n",
        "    repo_id=\"<namespace>/dummy-model\",\n",
        ")"
      ],
      "metadata": {
        "id": "tbEJaqpARwy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "upload file `config.json` available at `<path_to_file>` to the root of the repository as `config.json` ‚û° `dummy-model` repository"
      ],
      "metadata": {
        "id": "-UB_jWrjR6Z2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. The `Repository` class**\n",
        "Using this class **requires having git and git-lfs** installed and set up before begin."
      ],
      "metadata": {
        "id": "hkjhJhTrSPe9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---- **Initialising** the repository into a local folder **by cloning the remote repository**"
      ],
      "metadata": {
        "id": "D3J3fxzHSh44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import Repository\n",
        "\n",
        "repo = Repository(\"<path_to_dummy_folder>\", clone_from=\"<namespace>/dummy-model\")"
      ],
      "metadata": {
        "id": "03cb7Qe4SqOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This created the folder `<path_to_dummy_folder>` in our working directory. And only contains the `.gitattributes` files."
      ],
      "metadata": {
        "id": "TDf5WrWFS1ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leveraging the methods:\n",
        "- `repo.git_pull()`\n",
        "- `repo.git_add()`\n",
        "- `repo.git_commit()`\n",
        "- `repo.git_push()`\n",
        "- r`epo.git_tag()`"
      ],
      "metadata": {
        "id": "GuhDRi5LS7pN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---- `.pull()` [make sure, local clone is up to date -- latest changes]"
      ],
      "metadata": {
        "id": "P3EbqrmUTL06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo.git_pull()"
      ],
      "metadata": {
        "id": "7cuqDUK2TVxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---- **save the model and tokenizer files**"
      ],
      "metadata": {
        "id": "q9uigz00TdLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"<path_to_dummy_folder>\")\n",
        "tokenizer.save_pretrained(\"<path_to_dummy_folder>\")"
      ],
      "metadata": {
        "id": "B3uwdmuYTgWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The <path_to_dummy_folder> now contains all the model and tokenizer files.\n",
        "\n",
        "Next, is follow the usual git workflow (adding files to the staging area, commiting, and pushin to the hub)"
      ],
      "metadata": {
        "id": "uzBDQHjaTlZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo.git_add()\n",
        "repo.git_commit(\"Add model and tokenizer files\")\n",
        "repo.git_push()"
      ],
      "metadata": {
        "id": "OQbuYIFVTt2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. The `git-based` approach**\n",
        "Using this class requires having git and git-lfs installed and set up before begin."
      ],
      "metadata": {
        "id": "9iGn55ScTxRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---- **Inializing `git-lfs`**"
      ],
      "metadata": {
        "id": "H55CQNC_UH8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "git lfs install"
      ],
      "metadata": {
        "id": "DXByMA-HUL3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---- **Clone model repository**"
      ],
      "metadata": {
        "id": "yUvuZ_zaUNu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "git clone https://huggingface.co/<namespace>/<your-model-id>\n",
        "\n",
        "## namespace = <username>\n",
        "## model-id  = <model-that-you-used>"
      ],
      "metadata": {
        "id": "HHV8ugACUQ7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---- cd to the folder and look at the contents"
      ],
      "metadata": {
        "id": "2yO9H7xdUuEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd dummy && ls"
      ],
      "metadata": {
        "id": "AJxMvkEpUzP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---- **Generate code to commit** to dummy repository"
      ],
      "metadata": {
        "id": "3ivb30jZVY0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "checkpoint = \"camembert-base\"\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Do whatever with the model, train it, fine-tune it...\n",
        "\n",
        "model.save_pretrained(\"<path_to_dummy_folder>\")\n",
        "tokenizer.save_pretrained(\"<path_to_dummy_folder>\")"
      ],
      "metadata": {
        "id": "N95MnZDlVhEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------- list of the `dummy` folder"
      ],
      "metadata": {
        "id": "qIR_1LxdVxph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls\n",
        "#config.json  pytorch_model.bin  README.md  sentencepiece.bpe.model  special_tokens_map.json tokenizer_config.json  tokenizer.json\n",
        "\n",
        "ls -lh\n",
        "# check the size of the model"
      ],
      "metadata": {
        "id": "b4vEkga5V36G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úèÔ∏è When creating the repository from the web interface, the *.gitattributes* file is automatically set up to consider files with certain extensions, such as *.bin* and *.h5*, as large files, and git-lfs will track them with no necessary setup on your side."
      ],
      "metadata": {
        "id": "RxX8PhePWGYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add to staging area\n",
        "git add .\n",
        "\n",
        "# File that currently staged\n",
        "git status\n",
        "\n",
        "# check that git-lfs tracking the correct file\n",
        "git lfs status\n",
        "## LFS is LargeFiles\n",
        "## Git is SmallerFiles\n",
        "\n",
        "# Commit and Push to huggingface\n",
        "git commit -m \"Messege\"\n",
        "git push"
      ],
      "metadata": {
        "id": "i2xwDExjWRXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Building a Model Card**\n",
        "It is the **central definition of the model, ensuring reusability by fellow community members** and **reproducibility of results**, and **providing a platform** on which other members may build their artifacts."
      ],
      "metadata": {
        "id": "8MSErpe7EqiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Documenting the training and evaluation process helps others **understand what to expect of a model** ‚Äî and **providing sufficient information** regarding the **data that was used** and the **preprocessing and postprocessing that were done** ensures that the limitations, biases, and contexts in which the model is and is not useful can be identified and understood.\n",
        "\n",
        "‚ö† **Therefore, creating a model card that clearly defines your model is a very important step**"
      ],
      "metadata": {
        "id": "hnMQ5mGcj6X6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model card usually starts with a very brief, high-level overview of what the model is for.\n",
        "- Model description\n",
        "- Intended uses & limitations\n",
        "- How to use\n",
        "- Limitations and bias\n",
        "- Training data\n",
        "- Training procedure\n",
        "- Evaluation results"
      ],
      "metadata": {
        "id": "YLtuv3gTkuvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Description**\n",
        "üìì Basic details about the model. Includes:\n",
        "1. Architecture version\n",
        "2. If it was introduced in a paper\n",
        "3. If an original implementation is available\n",
        "4. The author\n",
        "5. Any copyright\n",
        "6. General information about (the model, training procedures, parameters, disclaimer)"
      ],
      "metadata": {
        "id": "zX59NfA-k6lS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Intended uses & limitations**\n",
        "üìì Use cases the model is intended for:\n",
        "1. The language\n",
        "2. Fields and domains where it can be applied\n",
        "3. Document areas that are know to be out of scope for the model\n",
        "4. Where it is perform suboptimally"
      ],
      "metadata": {
        "id": "VM3zZP7xlpYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **How to Use**\n",
        "üìì Include some examples of how to use the model:\n",
        "1. Showcase usage of the `pipeline()` function\n",
        "2. Usage of the model and tokenizer classes\n",
        "3. And any other code (*helpful*)"
      ],
      "metadata": {
        "id": "IXZNDbVsmGnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training data**\n",
        "üìì Indicate which dataset(s) the model was trained on. A brief description of the dataset(s) is also welcome"
      ],
      "metadata": {
        "id": "x7A_JRsNmVnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training procedure**\n",
        "üìì Describe all the relevant aspects of training that are useful from a reproducibility perspective.\n",
        "1. Any preprocessing and postprocessing that were done on the data\n",
        "2. Details such as the number of:\n",
        "    - `epochs` the model was trained for,\n",
        "    - the `batch size`,\n",
        "    - the `learning rate`,\n",
        "    - and so on."
      ],
      "metadata": {
        "id": "7qABTjAjmcJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Variable and metrics**\n",
        "üìì Describe the **metrics** you use **for evaluation**, and the **different factors you are mesuring**.\n",
        "1. Mentioning which metric(s) were used, on which dataset and which dataset split,\n",
        "2. makes it easy to compare you model‚Äôs performance compared to that of other models.\n",
        "3. These should be informed by the previous sections, such as the intended users and use cases."
      ],
      "metadata": {
        "id": "506MtdSlmzt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation results**\n",
        "üìì Provide an **indication of how well the model performs on the evaluation dataset**. If the model uses a decision threshold, either provide the decision threshold used in the evaluation, or provide details on evaluation at different thresholds for the intended uses."
      ],
      "metadata": {
        "id": "GPuEWQ6CnOBc"
      }
    }
  ]
}