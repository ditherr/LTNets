{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Summarization**\n",
        "Transformer models can be used to **condense long documents into summaries**. This is one of the most challenging NLP tasks as it *requires a range of abilities*, such as **understanding long passages and generating coherent text that captures the main topics in a document**. However, when done well, text summarization is a powerful tool that can **speed up various business processes by relieving the burden of domain experts to read long documents in detail**."
      ],
      "metadata": {
        "id": "Hn2ojZeG41U0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll train a bilingual model for English and Spanish that can summarize customer review. As we’ll see, these summaries are concise because they’re learned from the titles that customers provide in their product reviews."
      ],
      "metadata": {
        "id": "E3TvRzIL-G-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece] datasets evaluate accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "C6eUdKoV-cBv",
        "outputId": "84aecfaf-8f75-440e-dd64-bd208975d167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.1+cu121)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[sentencepiece]) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2024.8.30)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading datasets-3.0.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.0.0 dill-0.3.8 evaluate-0.4.3 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preparing a Multilingual Corpus**\n",
        "We’ll use the **Multilingual Amazon Reviews Corpus** to create our *bilingual summarizer*. This corpus consists of Amazon product reviews in six languages and is typically used to benchmark multilingual classifiers. However, since each review is accompanied by a short title, we can use the titles as the target summaries for our model to learn from!\n"
      ],
      "metadata": {
        "id": "37--4qZp44Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "spanish_dataset = load_dataset(\"amazon_reviews_multi\", \"es\")\n",
        "english_dataset = load_dataset(\"amazon_reviews_multi\", \"en\")\n",
        "english_dataset"
      ],
      "metadata": {
        "id": "U1Ar43GG_yGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "DatasetDict({\n",
        "    train: Dataset({\n",
        "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
        "        num_rows: 200000\n",
        "    })\n",
        "    validation: Dataset({\n",
        "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
        "        num_rows: 5000\n",
        "    })\n",
        "    test: Dataset({\n",
        "        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],\n",
        "        num_rows: 5000\n",
        "    })\n",
        "})\n",
        "```"
      ],
      "metadata": {
        "id": "AV80Wj3o_ucU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, for each language there are 200,000 reviews for the `train` split, and 5,000 reviews for each of the `validation` and `test` splits. The **review information** we are interested in is contained in the `review_body` and `review_title` columns"
      ],
      "metadata": {
        "id": "tbXHsA0D_9MI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s take a look at a few examples by creating a simple function that takes a random sample from the training set"
      ],
      "metadata": {
        "id": "ogYjZS0EAHbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_samples(dataset, num_samples=3, seed=42):\n",
        "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
        "    for example in sample:\n",
        "        print(f\"\\n'>> Title: {example['review_title']}'\")\n",
        "        print(f\"'>> Review: {example['review_body']}'\")\n",
        "\n",
        "show_samples(english_dataset)\n",
        "# show_samples(spanish_dataset)"
      ],
      "metadata": {
        "id": "DZAsvjNXAJSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "'>> Title: Worked in front position, not rear'\n",
        "'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'\n",
        "\n",
        "'>> Title: meh'\n",
        "'>> Review: Does it’s job and it’s gorgeous but mine is falling apart, I had to basically put it together again with hot glue'\n",
        "\n",
        "'>> Title: Can\\'t beat these for the money'\n",
        "'>> Review: Bought this for handling miscellaneous aircraft parts and hanger \"stuff\" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\\'t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\\'s heavy duty enough to hold metal parts, but being made of plastic it\\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\\'t beat it. Best one of these I\\'ve bought to date-- and I\\'ve been using some version of these for over forty years.'\n",
        "```"
      ],
      "metadata": {
        "id": "B2xBRxmhAMaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This sample shows the diversity of reviews one typically finds online, ranging from positive to negative (and everything in between!). Training a summarization model on all 400,000 reviews would take far too long on a single GPU, so instead we’ll focus on **generating summaries for a single domain of products**. To get a feel for what domains we can choose from, let’s convert `english_dataset` to a `pandas.DataFrame` and **compute the number of reviews per product category**."
      ],
      "metadata": {
        "id": "l9cftwXtAwsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_dataset.set_format(\"pandas\")\n",
        "english_df = english_dataset[\"train\"][:]\n",
        "\n",
        "# Show counts for top 20 products\n",
        "english_df[\"product_category\"].value_counts()[:20]"
      ],
      "metadata": {
        "id": "GexuNOq5A6oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "home                      17679\n",
        "apparel                   15951\n",
        "wireless                  15717\n",
        "other                     13418\n",
        "beauty                    12091\n",
        "drugstore                 11730\n",
        "kitchen                   10382\n",
        "toy                        8745\n",
        "sports                     8277\n",
        "automotive                 7506\n",
        "lawn_and_garden            7327\n",
        "home_improvement           7136\n",
        "pet_products               7082\n",
        "digital_ebook_purchase     6749\n",
        "pc                         6401\n",
        "electronics                6186\n",
        "office_product             5521\n",
        "shoes                      5197\n",
        "grocery                    4730\n",
        "book                       3756\n",
        "Name: product_category, dtype: int64\n",
        "```"
      ],
      "metadata": {
        "id": "6eq0_rMGA7gp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most popular products in the English dataset are about household items, clothing, and wireless electronics. To stick with the Amazon theme, though, **let’s focus on summarizing book reviews** — after all, this is what the company was founded on! We can see two product categories that fit the bill (`book` and `digital_ebook_purchase`), so let’s filter the datasets in both languages for just these products."
      ],
      "metadata": {
        "id": "5Q19VgsLBJoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_books(example):\n",
        "    return (\n",
        "        example[\"product_category\"] == \"book\"\n",
        "        or example[\"product_category\"] == \"digital_ebook_purchase\"\n",
        "    )"
      ],
      "metadata": {
        "id": "CiftM3gVBOEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_dataset.reset_format()"
      ],
      "metadata": {
        "id": "GhXmLCoxBTIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spanish_books = spanish_dataset.filter(filter_books)\n",
        "english_books = english_dataset.filter(filter_books)\n",
        "\n",
        "show_samples(english_books)"
      ],
      "metadata": {
        "id": "gPNY9nakBWSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "'>> Title: I\\'m dissapointed.'\n",
        "'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\\'m dissapointed.'\n",
        "\n",
        "'>> Title: Good art, good price, poor design'\n",
        "'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'\n",
        "\n",
        "'>> Title: Helpful'\n",
        "'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'\n",
        "```"
      ],
      "metadata": {
        "id": "S4XGgFJDBYEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, we can see that the reviews are not strictly about books and might refer to things like calendars and electronic applications such as OneNote. Nevertheless, the domain seems about right to train a summarization model on.\n",
        "\n",
        "Before we look at various models that are suitable for this task, we have one last bit of data preparation to do: **combining the English and Spanish reviews as a single DatasetDict object**. 🤗 Datasets provides a handy `concatenate_datasets()` function that (as the name suggests) *will stack two Dataset objects on top of each other*.\n",
        "\n",
        "So, to create our bilingual dataset, we’ll loop over each split, concatenate the datasets for that split, and shuffle the result to ensure our model doesn’t overfit to a single language"
      ],
      "metadata": {
        "id": "5-PWqGprBlEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import concatenate_datasets, DatasetDict\n",
        "\n",
        "books_dataset = DatasetDict()\n",
        "\n",
        "for split in english_books.keys():\n",
        "    books_dataset[split] = concatenate_datasets(\n",
        "        [english_books[split], spanish_books[split]]\n",
        "    )\n",
        "    books_dataset[split] = books_dataset[split].shuffle(seed=42)\n",
        "\n",
        "# Peek at a few examples\n",
        "show_samples(books_dataset)"
      ],
      "metadata": {
        "id": "BEa0rS87ByIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "'>> Title: Easy to follow!!!!'\n",
        "'>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'\n",
        "\n",
        "'>> Title: PARCIALMENTE DAÑADO'\n",
        "'>> Review: Me llegó el día que tocaba, junto a otros libros que pedí, pero la caja llegó en mal estado lo cual dañó las esquinas de los libros porque venían sin protección (forro).'\n",
        "\n",
        "'>> Title: no lo he podido descargar'\n",
        "'>> Review: igual que el anterior'\n",
        "```"
      ],
      "metadata": {
        "id": "GLQTaLNxC8kw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a training corpus, one final thing to check is the **distribution of words in the reviews and their titles**. This is especially important for summarization tasks, where **short reference summaries in the data can bias the model to only output one or two words in the generated summaries**."
      ],
      "metadata": {
        "id": "IBKUfM-JDm4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To deal with this, we’ll **filter out the examples with very short titles so that our model can produce more interesting summaries**. Since we’re dealing with English and Spanish texts, we can use a `rough heuristic` **to split the titles on whitespace** and then use our trusty `Dataset.filter()` method."
      ],
      "metadata": {
        "id": "epicemLoDyZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "books_dataset = books_dataset.filter(lambda x: len(x[\"review_title\"].split()) > 2)"
      ],
      "metadata": {
        "id": "HyUTNeiPD8rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Models for Text Summarization**\n",
        "Accordingly, most Transformer models for summarization adopt the *encoder-decoder architecture*, although there are some exceptions like the GPT family of models which can also be used for summarization in few-shot settings."
      ],
      "metadata": {
        "id": "RQrGoB0047m4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The majority of Transformer models for summarization (and indeed most NLP tasks) are **monolingual**. This is great if your task is in a “high-resource” language like English or German, but less so for the thousands of other languages in use across the world. Fortunately, there is a class of multilingual Transformer models, like `mT5` and `mBART`, that come to the rescue. These models are **pretrained using language modeling, but with a twist: instead of training on a corpus of one language, they are trained jointly on texts in over 50 languages at once!**"
      ],
      "metadata": {
        "id": "U01COJdHFiUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll focus on `mT5`, an interesting architecture based on `T5` that was pretrained in a **text-to-text framework**. In `T5`, every NLP task is **formulated in terms of a prompt prefix** like `summarize:` which **conditions the model to adapt the generated text to the prompt**."
      ],
      "metadata": {
        "id": "djcPHPl_F1EB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5-dark.svg)"
      ],
      "metadata": {
        "id": "5C23ZAMWG8xQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "mT5 doesn’t use prefixes, but shares **much of the versatility of T5 and has the advantage of being multilingual**."
      ],
      "metadata": {
        "id": "Cqfm86lfJDvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing the data**\n",
        "Our next task is to **tokenize and encode our reviews and their titles**. As usual, we begin by loading the tokenizer associated with the pretrained model checkpoint. We’ll use `mt5-small` as our checkpoint so we can fine-tune the model in a reasonable amount of time."
      ],
      "metadata": {
        "id": "u1CaBSq14-M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"google/mt5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "oy0YsKmOJWZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 💡 In the early stages of your NLP projects, a good practice is to **train a class of “small” models on a small sample of data**. This allows you to **debug and iterate faster toward an end-to-end workflow**. Once you are confident in the results, you can always scale up the model by simply changing the model checkpoint!\n",
        "\n"
      ],
      "metadata": {
        "id": "96djvhEhJZES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"I loved reading the Hunger Games!\")\n",
        "inputs"
      ],
      "metadata": {
        "id": "RtvUn4mBJiro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
        "```"
      ],
      "metadata": {
        "id": "qn2qsM0xJjsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see the familiar `input_ids` and `attention_mask` that we encountered in our first fine-tuning. Let’s decode these input `IDs` with the tokenizer’s `convert_ids_to_tokens()` function to see what kind of tokenizer we’re dealing with."
      ],
      "metadata": {
        "id": "-9B9dQH-JrrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens(inputs.input_ids)"
      ],
      "metadata": {
        "id": "7FMl4sNgJyc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "['▁I', '▁', 'loved', '▁reading', '▁the', '▁Hung', 'er', '▁Games', '</s>']\n",
        "```"
      ],
      "metadata": {
        "id": "KqIZb4CbJy1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The special Unicode character `▁` and end-of-sequence token `</s>` indicate that we’re dealing with the `SentencePiece` tokenizer, which is based on the Unigram segmentation algorithm. Unigram is especially **useful for multilingual corpora** since it allows SentencePiece to be **agnostic about accents, punctuation, and the fact that many languages, like Japanese, do not have whitespace characters**."
      ],
      "metadata": {
        "id": "hxasBIwuKSFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tokenize our corpus, we have to **deal with a subtlety associated with summarization**: because our labels are also text, it is possible that they exceed the model’s maximum context size. This means we need to **apply truncation to both the reviews and their titles to ensure we don’t pass excessively long inputs to our model**. The tokenizers in 🤗 Transformers provide a nifty `text_target` argument that allows you to tokenize the labels in parallel to the inputs."
      ],
      "metadata": {
        "id": "TW3Lb8T9LxEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 512\n",
        "max_target_length = 30\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(\n",
        "        examples[\"review_body\"],\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        examples[\"review_title\"], max_length=max_target_length, truncation=True\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "ao9p36HqL4TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing we’ve done is define values for `max_input_length` and `max_target_length`, which set the upper limits for how long our reviews and titles can be. Since the review body is typically much larger than the title, we’ve scaled these values accordingly."
      ],
      "metadata": {
        "id": "ndOSZ7BpMO6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With `preprocess_function()`, it is then a simple matter to tokenize the whole corpus using the handy `Dataset.map()` function"
      ],
      "metadata": {
        "id": "yRE2IqM3MU28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = books_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "Dvvmk-UYMZGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the corpus has been preprocessed, let’s take a look at some **metrics** that are commonly used for summarization. As we’ll see, **there is no silver bullet when it comes to measuring the quality of machine-generated text**."
      ],
      "metadata": {
        "id": "v5m5A4uFMd63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Metrics for Text Summarization**\n",
        "For summarization, one of the most commonly used metrics is the `ROUGE score` (short for Recall-Oriented Understudy for Gisting Evaluation). The basic idea behind this metric is **to compare a generated summary against a set of reference summaries that are typically created by humans.**"
      ],
      "metadata": {
        "id": "iUnigUyQ5AFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Example:"
      ],
      "metadata": {
        "id": "uQdhNc4fNTTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_summary = \"I absolutely loved reading the Hunger Games\"\n",
        "reference_summary = \"I loved reading the Hunger Games\""
      ],
      "metadata": {
        "id": "Ip2nHm2gNVG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to compare them could be to count the number of overlapping words, which in this case would be 6. However, this is a bit crude, so instead **ROUGE** is based on computing the *precision* and *recall* scores for the overlap"
      ],
      "metadata": {
        "id": "MYlsoBRVNiWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For ROUGE, recall measures how much of the reference summary is captured by the generated one. For precision, it measures how much of the generated summary was relevant."
      ],
      "metadata": {
        "id": "R3zozPq1Noo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "wJOA1HdkOu78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "rouge_score = evaluate.load(\"rouge\")"
      ],
      "metadata": {
        "id": "6C4sDxYkOyZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate all the metrics at once\n",
        "scores = rouge_score.compute(\n",
        "    predictions=[generated_summary], references=[reference_summary]\n",
        ")\n",
        "scores"
      ],
      "metadata": {
        "id": "vs_73VSHOyUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),\n",
        " 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),\n",
        " 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),\n",
        " 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}\n",
        "```"
      ],
      "metadata": {
        "id": "dzrNcKbBO55k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, 🤗 Datasets actually **computes confidence intervals for precision, recall, and F1-score**; these are the `low`, `mid`, and `high` attributes you can see here. Moreover, 🤗 Datasets computes a variety of **ROUGE** scores which are based on different types of text granularity when comparing the generated and reference summaries. The `rouge1` variant is the overlap of `unigrams` — this is just a fancy way of saying the overlap of words and is exactly the metric we’ve discussed above."
      ],
      "metadata": {
        "id": "jK8jQwQ6PPiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores[\"rouge1\"].mid"
      ],
      "metadata": {
        "id": "Jn9vOtG6PYt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Score(precision=0.86, recall=1.0, fmeasure=0.92)\n",
        "```"
      ],
      "metadata": {
        "id": "zHG7KQFfPZve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, the precision and recall numbers match up! Now what about those other ROUGE scores?\n",
        "- `rouge2` measures the overlap between `bigrams` (think the overlap of pairs of words),\n",
        "- `rougeL` and `rougeLsum` measure the longest matching sequences of words by looking for the longest common substrings in the generated and reference summaries.\n",
        "- The “sum” in `rougeLsum` refers to the fact that this metric is **computed over a whole summary**, while `rougeL` is computed as the **average over individual sentences**."
      ],
      "metadata": {
        "id": "Vej6M-UXPlZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll use these ROUGE scores to **track the performance of our model**, but before doing that let’s do something every good NLP practitioner should do: **create a strong, yet simple baseline!**"
      ],
      "metadata": {
        "id": "_rxbrRuDQDR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating a strong baseline**\n",
        "A **common baseline** for text summarization is to simply take the **first three sentences of an article**, often called the **`lead-3 baseline`**. We could use full stops to track the sentence boundaries, but this will fail on acronyms like “U.S.” or “U.N.” — so instead we’ll use the nltk library, which includes a better algorithm to handle these cases"
      ],
      "metadata": {
        "id": "3pK_w6FN5DeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "u_JT-F8YQcHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "DfdCp0kkQdVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we import the sentence tokenizer from `nltk` and create a simple function to **extract the first three sentences in a review**. The convention in text summarization is to separate each summary with a newline, so let’s also include this and test it on a training example"
      ],
      "metadata": {
        "id": "P9HpAC5tQj6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def three_sentence_summary(text):\n",
        "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
        "\n",
        "print(three_sentence_summary(books_dataset[\"train\"][1][\"review_body\"]))"
      ],
      "metadata": {
        "id": "84_uvG4XQmw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "'I grew up reading Koontz, and years ago, I stopped,convinced i had \"outgrown\" him.'\n",
        "'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.'\n",
        "'She found Strangers.'\n",
        "```"
      ],
      "metadata": {
        "id": "1LjQ9Lp5Q5D8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This seems to work, so let’s now implement a function that **extracts these `“summaries”` from a dataset and computes the `ROUGE` scores for the baseline**"
      ],
      "metadata": {
        "id": "7S9Te0uqQ-Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_baseline(dataset, metric):\n",
        "    summaries = [three_sentence_summary(text) for text in dataset[\"review_body\"]]\n",
        "    return metric.compute(predictions=summaries, references=dataset[\"review_title\"])"
      ],
      "metadata": {
        "id": "K24mr3cZRC_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the rouge score\n",
        "import pandas as pd\n",
        "\n",
        "score = evaluate_baseline(books_dataset[\"validation\"], rouge_score)\n",
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)\n",
        "rouge_dict"
      ],
      "metadata": {
        "id": "4whIH07aRL1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}\n",
        "```"
      ],
      "metadata": {
        "id": "7OFu7F9eRWJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the `rouge2` score is significantly lower than the rest; this likely reflects the fact that review titles are typically concise and so the lead-3 baseline is too verbose."
      ],
      "metadata": {
        "id": "wH5sJoQqRc47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fine-tuning mT5 with the Trainer API**\n",
        "Fine-tuning a model for summarization is very similar to the other tasks we’ve covered in this chapter. The first thing we need to do is load the pretrained model from the `mt5-small` checkpoint. Since summarization is a sequence-to-sequence task, we can load the model with the `AutoModelForSeq2SeqLM` class, which will automatically download and cache the weights"
      ],
      "metadata": {
        "id": "ZVBRWcEd5F7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "4S3U6YLYTDsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "VwiN8wRvTJj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "generate summaries in order to compute **ROUGE** scores during training. Fortunately, 🤗 Transformers provides dedicated `Seq2SeqTrainingArguments` and `Seq2SeqTrainer` classes that can do this for us automatically!"
      ],
      "metadata": {
        "id": "UDXq5TlsTN4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "batch_size = 8\n",
        "num_train_epochs = 8\n",
        "# Show the training loss with every epoch\n",
        "logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=f\"{model_name}-finetuned-amazon-en-es\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5.6e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    predict_with_generate=True,\n",
        "    logging_steps=logging_steps,\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "H7vADmGiTVXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the `predict_with_generate` argument has been set to indicate that we **should generate summaries during evaluation so that we can compute ROUGE scores for each epoch**. As discussed, the decoder performs inference by predicting tokens one by one, and this is implemented by the model’s `generate()` method. Setting `predict_with_generate=True` tells the `Seq2SeqTrainer` to **use that method for evaluation**. We’ve also adjusted some of the default hyperparameters, like the learning rate, number of epochs, and weight decay, and we’ve set the `save_total_limit` option to only save up to 3 checkpoints during training — this is because even the “small” version of mT5 uses around a GB of hard drive space, and we can save a bit of room by limiting the number of copies we save."
      ],
      "metadata": {
        "id": "ejcHz-nzTuCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next thing we need to do is provide the trainer with a `compute_metrics()` function so that we can evaluate our model during training. For summarization this is a bit more involved than simply calling `rouge_score.compute()` on the model’s predictions, since we need to **decode the outputs and labels into text before we can compute the ROUGE scores**. The following function does exactly that, and also makes use of the `sent_tokenize()` function from `nltk` to separate the summary sentences with newlines"
      ],
      "metadata": {
        "id": "0Jq-9fpVU9d7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    # Decode generated summaries into text\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    # Decode reference summaries into text\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # ROUGE expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    # Compute ROUGE scores\n",
        "    result = rouge_score.compute(\n",
        "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
        "    )\n",
        "    # Extract the median scores\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ],
      "metadata": {
        "id": "TbV-XQzqVKFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to define a **data collator** for our sequence-to-sequence task. Since `mT5` is an encoder-decoder Transformer model, one subtlety with **preparing our batches is that during decoding we need to shift the labels to the right by one**. This is required to ensure that the **decoder only sees the previous ground truth labels and not the current or future ones, which would be easy for the model to memorize**."
      ],
      "metadata": {
        "id": "rVsftzANVqfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "cWUuDqyhWElN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = tokenized_datasets.remove_columns(\n",
        "    books_dataset[\"train\"].column_names\n",
        ")"
      ],
      "metadata": {
        "id": "ZGD38fV8WJg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the collator expects a list of `dicts`, where each `dict` represents a single example in the dataset, we also need to wrangle the data into the expected format before passing it to the data collato"
      ],
      "metadata": {
        "id": "du4LUS7mWN4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = [tokenized_datasets[\"train\"][i] for i in range(2)]\n",
        "data_collator(features)"
      ],
      "metadata": {
        "id": "EjiG1kx0WQPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
        "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,\n",
        "            772,    281,    772,   1617,    263,    305,  14701,    260,   1385,\n",
        "           3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,\n",
        "            260,      1,      0,      0,      0,      0,      0,      0],\n",
        "        [   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,\n",
        "          53198,    276,    259,  74060,    263,    260,    459,  25640,    776,\n",
        "           2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,\n",
        "           1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],\n",
        "        [  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],\n",
        "        [    0,   259, 27531, 13483,   259,  7505]])}\n",
        "```"
      ],
      "metadata": {
        "id": "fFnNKPweWRoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main thing to notice here is that the first example is longer than the second one, so the `input_ids` and `attention_mask` of the second example have been padded on the right with a `[PAD]` token (whose ID is 0).\n",
        "\n",
        "Similarly, we can see that the labels have been padded with `-100`s, to make sure the padding tokens are ignored by the loss function. And finally, we can see a new `decoder_input_ids` which has shifted the labels to the right by inserting a `[PAD]` token in the first entry."
      ],
      "metadata": {
        "id": "tCTUaSeWWzcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "WYxtv4RbXPcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "fGOtLvbIXQrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training, you should see the training `loss` decrease and the `ROUGE` scores increase with each epoch. Once the training is complete, you can see the final **ROUGE** scores by running `trainer.evaluate()`"
      ],
      "metadata": {
        "id": "1tHUmolxXT6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "Cne3Vn00XYym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "{'eval_loss': 3.028524398803711,\n",
        " 'eval_rouge1': 16.9728,\n",
        " 'eval_rouge2': 8.2969,\n",
        " 'eval_rougeL': 16.8366,\n",
        " 'eval_rougeLsum': 16.851,\n",
        " 'eval_gen_len': 10.1597,\n",
        " 'eval_runtime': 6.1054,\n",
        " 'eval_samples_per_second': 38.982,\n",
        " 'eval_steps_per_second': 4.914}\n",
        "```"
      ],
      "metadata": {
        "id": "QaOnRDRfXZcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(commit_message=\"Training complete\", tags=\"summarization\")"
      ],
      "metadata": {
        "id": "Yk9eQR7EXdDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fine-tuning mT5 with Accelerate**\n",
        "The main differences will be the need to explicitly** generate our summaries during training** and **define how we compute the ROUGE scores** (recall that the `Seq2SeqTrainer` took care of the generation for us)"
      ],
      "metadata": {
        "id": "TcBpR5wM5LnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preparing everything for training**"
      ],
      "metadata": {
        "id": "_8O8FpfB5Njy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "h_jDgjd1XjMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "namxpT4zXuoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "NMOXCe0xXx5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "6F3T5sXWX1zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator()\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")"
      ],
      "metadata": {
        "id": "b8Jh2URqX3xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we’ve prepared our objects, there are three remaining things to do:\n",
        "\n",
        "- Define the learning rate schedule.\n",
        "- Implement a function to post-process the summaries for evaluation.\n",
        "- Create a repository on the Hub that we can push our model to."
      ],
      "metadata": {
        "id": "tgWz7fMIX6Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = 10\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ],
      "metadata": {
        "id": "kladIiHsX-El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import get_full_repo_name\n",
        "\n",
        "model_name = \"test-bert-finetuned-squad-accelerate\"\n",
        "repo_name = get_full_repo_name(model_name)\n",
        "repo_name"
      ],
      "metadata": {
        "id": "Zvh6KwtVYliF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import Repository\n",
        "\n",
        "output_dir = \"results-mt5-finetuned-squad-accelerate\"\n",
        "repo = Repository(output_dir, clone_from=repo_name)"
      ],
      "metadata": {
        "id": "isiHje-8Yn3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For post-processing, we need a function that **splits the generated summaries into sentences that are separated by newlines**. This is the format the ROUGE metric expects."
      ],
      "metadata": {
        "id": "BIuXRr3tYDay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training Loop**\n",
        "The training loop for summarization is quite similar to the other 🤗 Accelerate examples that we’ve encountered and is roughly split into four main steps:\n",
        "\n",
        "1. Train the model by iterating over all the examples in `train_dataloader` for each epoch.\n",
        "2. **Generate model summaries** at the end of each epoch, by first generating the tokens and then decoding them (and the reference summaries) into text.\n",
        "3. Compute the **ROUGE** scores using the same techniques we saw earlier.\n",
        "4. Save the checkpoints and push everything to the Hub. Here we rely on the nifty `blocking=False` argument of the `Repository` object so that we can push the checkpoints per epoch asynchronously. This allows us to continue training without having to wait for the somewhat slow upload associated with a `GB`-sized model!"
      ],
      "metadata": {
        "id": "jM74923a5Qey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [label.strip() for label in labels]\n",
        "\n",
        "    # ROUGE expects a newline after each sentence\n",
        "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "    return preds, labels"
      ],
      "metadata": {
        "id": "JtHL3ZNBYFk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
        "                batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "            )\n",
        "\n",
        "            generated_tokens = accelerator.pad_across_processes(\n",
        "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
        "            )\n",
        "            labels = batch[\"labels\"]\n",
        "\n",
        "            # If we did not pad to max length, we need to pad the labels too\n",
        "            labels = accelerator.pad_across_processes(\n",
        "                batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
        "            labels = accelerator.gather(labels).cpu().numpy()\n",
        "\n",
        "            # Replace -100 in the labels as we can't decode them\n",
        "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "            if isinstance(generated_tokens, tuple):\n",
        "                generated_tokens = generated_tokens[0]\n",
        "            decoded_preds = tokenizer.batch_decode(\n",
        "                generated_tokens, skip_special_tokens=True\n",
        "            )\n",
        "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            decoded_preds, decoded_labels = postprocess_text(\n",
        "                decoded_preds, decoded_labels\n",
        "            )\n",
        "\n",
        "            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    result = rouge_score.compute()\n",
        "    # Extract the median ROUGE scores\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    print(f\"Epoch {epoch}:\", result)\n",
        "\n",
        "    # Save and upload\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "    if accelerator.is_main_process:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        repo.push_to_hub(\n",
        "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
        "        )"
      ],
      "metadata": {
        "id": "mVFYZH2DZH7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005}\n",
        "Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306}\n",
        "Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468}\n",
        "Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518}\n",
        "Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029}\n",
        "Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913}\n",
        "Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701}\n",
        "Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194}\n",
        "Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744}\n",
        "Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509}\n",
        "```"
      ],
      "metadata": {
        "id": "S54Y7804ZKLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using fine-tuned model**"
      ],
      "metadata": {
        "id": "0g2BmILJ5SQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "hub_model_id = \"huggingface-course/mt5-small-finetuned-amazon-en-es\"\n",
        "summarizer = pipeline(\"summarization\", model=hub_model_id)"
      ],
      "metadata": {
        "id": "d2MThMFiwap9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can feed some examples from the test set (which the model has not seen) to our pipeline to get a feel for the quality of the summaries. First let’s implement a simple function to show the review, title, and generated summary together"
      ],
      "metadata": {
        "id": "Ykx7MNq5wdf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_summary(idx):\n",
        "    review = books_dataset[\"test\"][idx][\"review_body\"]\n",
        "    title = books_dataset[\"test\"][idx][\"review_title\"]\n",
        "    summary = summarizer(books_dataset[\"test\"][idx][\"review_body\"])[0][\"summary_text\"]\n",
        "    print(f\"'>>> Review: {review}'\")\n",
        "    print(f\"\\n'>>> Title: {title}'\")\n",
        "    print(f\"\\n'>>> Summary: {summary}'\")"
      ],
      "metadata": {
        "id": "J9nWBJtSwicv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_summary(100)"
      ],
      "metadata": {
        "id": "ikBOS3rawl5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesn’t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. It’s also really expensive for what it is.'\n",
        "\n",
        "'>>> Title: Not impressed at all... buy something else'\n",
        "\n",
        "'>>> Summary: Nothing special at all about this product'\n",
        "```"
      ],
      "metadata": {
        "id": "MHa9ZBevwmxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is not too bad! We can see that our model has actually been able to perform abstractive summarization by augmenting parts of the review with new words. And perhaps the coolest aspect of our model is that it is bilingual, so we can also generate summaries of Spanish review"
      ],
      "metadata": {
        "id": "4Xzalkidwviy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_summary(0)"
      ],
      "metadata": {
        "id": "bTziOfskwwJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada'\n",
        "\n",
        "'>>> Title: Buena literatura para adolescentes'\n",
        "\n",
        "'>>> Summary: Muy facil de leer'\n",
        "```"
      ],
      "metadata": {
        "id": "vGxlPZBHwxkV"
      }
    }
  ]
}